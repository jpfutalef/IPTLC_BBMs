{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# BBMs for the Optimal Power Flow\n",
    "\n",
    "In this script, we aim to construct Black-Box Models (BBMs) (i.e., data-driven models) for the Optimal Power Flow (OPF) problem. We use the data stored in the 'data/OPF' folder, containing tables with the OPF IO data.\n",
    "\n",
    "In general terms, the OPF problem is a non-linear mapping from the input space (i.e., the power injections) to the output space (i.e., the voltage magnitudes and angles): $$ \\mathbf{y} = \\mathbf{f}(\\mathbf{E}, \\mathbf{\\Pi}), $$ where $\\mathbf{E}$ is the vector of power injections and $\\mathbf{\\Pi}$ is the vector of active and reactive power demands, $\\mathbf{\\Pi}$ is a vector of the binary status of generators, transmission lines, and transformers (i.e., 1 \"on\" and 0 \"off\"), $\\mathbf{f}(\\cdot)$ is the OPF function, and $\\mathbf{y}$ is the vector of optimal voltage magnitudes and angles."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: D:\\projects\\IPTLC_BBMs\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "from importlib import reload\n",
    "\n",
    "import greyboxmodels.bbmcpsmodels.creator as creator\n",
    "import greyboxmodels.bbmcpsmodels.cyber.OPF as opf_bbm\n",
    "\n",
    "\n",
    "# Set the working directory\n",
    "os.chdir(\"D:/projects/IPTLC_BBMs/\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = creator.get_device()\n",
    "print(f\"Device: {device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T09:32:57.475218100Z",
     "start_time": "2024-02-29T09:32:57.455263500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Specify the paths\n",
    "datasets_folder = Path(\"data/OPF/20240226_184851\")\n",
    "\n",
    "# # Print the available datasets\n",
    "# print(\"Available datasets:\")\n",
    "# for path in datasets_folder.iterdir():\n",
    "#     if path.is_dir() and \"OPF\" in path.name:\n",
    "#         print(path.name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T09:32:58.854876700Z",
     "start_time": "2024-02-29T09:32:58.825015200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup the BBM creator"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T12:50:42.543453200Z",
     "start_time": "2024-01-30T12:50:42.370708400Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Create the BBMCreator\n",
    "reload(creator)\n",
    "BBM_creator = creator.BBMCreator()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T09:33:00.165363100Z",
     "start_time": "2024-02-29T09:33:00.148726200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: data\\OPF\\20240226_184851\n",
      "---- Dataset loaded ----\n",
      "Input shape: (8064, 51)\n",
      "Output shape: (8064, 9)\n",
      "---- Removing NaNs ----\n",
      "Rows to delete: []\n",
      "Input shape: (8064, 51)\n",
      "Output shape: (8064, 9)\n",
      "---- Converting to torch tensors ----\n",
      "---- Dataset loaded! ----\n"
     ]
    }
   ],
   "source": [
    "# Set up the dataset\n",
    "dataset_name = \"MinMaxNormalizedOPF\"\n",
    "loaders = creator.setup_datasets(datasets_folder,\n",
    "                                 dataset_name,\n",
    "                                 remove_nans=True,\n",
    "                                 ratios=(0.70, 0.15, 0.15),\n",
    "                                 batch_size=32,\n",
    "                                 input_name=\"opf_inputs_minmax_normalized.npy\",\n",
    "                                 output_name=\"opf_outputs_minmax_normalized.npy\")\n",
    "BBM_creator.set_dataloaders(*loaders)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T09:33:00.664228Z",
     "start_time": "2024-02-29T09:33:00.574775600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BBM 1: a two-layer feedforward neural network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "input_size = loaders[0].dataset[0][0].shape[0]\n",
    "output_size = loaders[0].dataset[0][1].shape[0]\n",
    "\n",
    "BBM_creator.instantiate_model(opf_bbm.BBM1_SimpleNet, input_size, output_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T13:48:00.323681700Z",
     "start_time": "2024-02-28T13:48:00.134822900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Training model 'BBM1_SimpleNet' ------\n",
      "Models and summary will be save to 'models/' (will be created if it does not exist)\n",
      "    - Model path: models\\BBM1_SimpleNet_MinMaxNormalizedOPF_20240228-144804.pt\n",
      "    - Summary path: models/models_summary.csv\n",
      "Training on cuda:0\n",
      "Training starts in:  00:00\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 (Loss - Train: 7.03e-03, Best val: N/A): 100%|██████████| 1109/1109 [00:07<00:00, 142.15it/s]\n",
      "Epoch 2/100 (Loss - Train: 2.49e-03, Best val: 2.79e-03): 100%|██████████| 1109/1109 [00:04<00:00, 271.05it/s]\n",
      "Epoch 3/100 (Loss - Train: 2.01e-03, Best val: 2.13e-03): 100%|██████████| 1109/1109 [00:04<00:00, 268.98it/s]\n",
      "Epoch 4/100 (Loss - Train: 1.68e-03, Best val: 1.69e-03): 100%|██████████| 1109/1109 [00:04<00:00, 259.73it/s]\n",
      "Epoch 5/100 (Loss - Train: 1.41e-03, Best val: 1.41e-03): 100%|██████████| 1109/1109 [00:04<00:00, 263.50it/s]\n",
      "Epoch 6/100 (Loss - Train: 1.29e-03, Best val: 1.29e-03): 100%|██████████| 1109/1109 [00:04<00:00, 264.76it/s]\n",
      "Epoch 7/100 (Loss - Train: 1.15e-03, Best val: 1.13e-03): 100%|██████████| 1109/1109 [00:04<00:00, 270.76it/s]\n",
      "Epoch 8/100 (Loss - Train: 1.07e-03, Best val: 1.00e-03): 100%|██████████| 1109/1109 [00:04<00:00, 265.86it/s]\n",
      "Epoch 9/100 (Loss - Train: 9.92e-04, Best val: 9.38e-04): 100%|██████████| 1109/1109 [00:04<00:00, 256.42it/s]\n",
      "Epoch 10/100 (Loss - Train: 9.40e-04, Best val: 8.93e-04): 100%|██████████| 1109/1109 [00:04<00:00, 245.36it/s]\n",
      "Epoch 11/100 (Loss - Train: 8.79e-04, Best val: 8.29e-04): 100%|██████████| 1109/1109 [00:04<00:00, 257.91it/s]\n",
      "Epoch 12/100 (Loss - Train: 8.29e-04, Best val: 7.76e-04): 100%|██████████| 1109/1109 [00:04<00:00, 259.38it/s]\n",
      "Epoch 13/100 (Loss - Train: 7.95e-04, Best val: 7.34e-04): 100%|██████████| 1109/1109 [00:04<00:00, 273.73it/s]\n",
      "Epoch 14/100 (Loss - Train: 7.61e-04, Best val: 7.34e-04): 100%|██████████| 1109/1109 [00:03<00:00, 278.48it/s]\n",
      "Epoch 15/100 (Loss - Train: 7.45e-04, Best val: 7.34e-04): 100%|██████████| 1109/1109 [00:03<00:00, 279.35it/s]\n",
      "Epoch 16/100 (Loss - Train: 7.32e-04, Best val: 7.02e-04): 100%|██████████| 1109/1109 [00:04<00:00, 259.71it/s]\n",
      "Epoch 17/100 (Loss - Train: 7.44e-04, Best val: 7.02e-04): 100%|██████████| 1109/1109 [00:03<00:00, 284.50it/s]\n",
      "Epoch 18/100 (Loss - Train: 7.26e-04, Best val: 6.76e-04): 100%|██████████| 1109/1109 [00:03<00:00, 292.69it/s]\n",
      "Epoch 19/100 (Loss - Train: 7.20e-04, Best val: 6.76e-04): 100%|██████████| 1109/1109 [00:04<00:00, 261.04it/s]\n",
      "Epoch 20/100 (Loss - Train: 7.13e-04, Best val: 6.22e-04): 100%|██████████| 1109/1109 [00:04<00:00, 251.15it/s]\n",
      "Epoch 21/100 (Loss - Train: 7.21e-04, Best val: 6.22e-04): 100%|██████████| 1109/1109 [00:04<00:00, 243.21it/s]\n",
      "Epoch 22/100 (Loss - Train: 7.08e-04, Best val: 6.22e-04): 100%|██████████| 1109/1109 [00:05<00:00, 221.36it/s]\n",
      "Epoch 23/100 (Loss - Train: 6.92e-04, Best val: 6.22e-04): 100%|██████████| 1109/1109 [00:04<00:00, 244.29it/s]\n",
      "Epoch 24/100 (Loss - Train: 7.06e-04, Best val: 6.22e-04): 100%|██████████| 1109/1109 [00:04<00:00, 249.87it/s]\n",
      "Epoch 25/100 (Loss - Train: 7.07e-04, Best val: 6.22e-04): 100%|██████████| 1109/1109 [00:04<00:00, 259.09it/s]\n",
      "Epoch 26/100 (Loss - Train: 7.12e-04, Best val: 6.22e-04): 100%|██████████| 1109/1109 [00:03<00:00, 282.34it/s]\n",
      "Epoch 27/100 (Loss - Train: 7.06e-04, Best val: 6.22e-04): 100%|██████████| 1109/1109 [00:04<00:00, 253.07it/s]\n",
      "Epoch 28/100 (Loss - Train: 6.95e-04, Best val: 6.22e-04): 100%|██████████| 1109/1109 [00:04<00:00, 247.98it/s]\n",
      "Epoch 29/100 (Loss - Train: 6.96e-04, Best val: 6.22e-04): 100%|██████████| 1109/1109 [00:04<00:00, 267.97it/s]\n",
      "Epoch 30/100 (Loss - Train: 6.94e-04, Best val: 6.22e-04): 100%|██████████| 1109/1109 [00:04<00:00, 258.96it/s]\n",
      "Epoch 31/100 (Loss - Train: 7.09e-04, Best val: 6.22e-04): 100%|██████████| 1109/1109 [00:04<00:00, 248.21it/s]\n",
      "Epoch 32/100 (Loss - Train: 6.83e-04, Best val: 6.22e-04): 100%|██████████| 1109/1109 [00:04<00:00, 244.53it/s]\n",
      "Epoch 33/100 (Loss - Train: 6.78e-04, Best val: 6.22e-04): 100%|██████████| 1109/1109 [00:04<00:00, 266.73it/s]\n",
      "Epoch 34/100 (Loss - Train: 6.88e-04, Best val: 6.22e-04): 100%|██████████| 1109/1109 [00:04<00:00, 238.55it/s]\n",
      "Epoch 35/100 (Loss - Train: 6.76e-04, Best val: 5.99e-04): 100%|██████████| 1109/1109 [00:04<00:00, 255.54it/s]\n",
      "Epoch 36/100 (Loss - Train: 6.80e-04, Best val: 5.99e-04): 100%|██████████| 1109/1109 [00:04<00:00, 243.74it/s]\n",
      "Epoch 37/100 (Loss - Train: 6.92e-04, Best val: 5.99e-04): 100%|██████████| 1109/1109 [00:04<00:00, 263.43it/s]\n",
      "Epoch 38/100 (Loss - Train: 6.72e-04, Best val: 5.99e-04): 100%|██████████| 1109/1109 [00:04<00:00, 272.46it/s]\n",
      "Epoch 39/100 (Loss - Train: 6.82e-04, Best val: 5.99e-04): 100%|██████████| 1109/1109 [00:04<00:00, 263.89it/s]\n",
      "Epoch 40/100 (Loss - Train: 6.77e-04, Best val: 5.99e-04): 100%|██████████| 1109/1109 [00:04<00:00, 253.69it/s]\n",
      "Epoch 41/100 (Loss - Train: 6.73e-04, Best val: 5.99e-04): 100%|██████████| 1109/1109 [00:04<00:00, 259.95it/s]\n",
      "Epoch 42/100 (Loss - Train: 6.77e-04, Best val: 5.99e-04): 100%|██████████| 1109/1109 [00:04<00:00, 236.05it/s]\n",
      "Epoch 43/100 (Loss - Train: 6.89e-04, Best val: 5.99e-04): 100%|██████████| 1109/1109 [00:03<00:00, 317.64it/s]\n",
      "Epoch 44/100 (Loss - Train: 6.71e-04, Best val: 5.99e-04): 100%|██████████| 1109/1109 [00:03<00:00, 317.89it/s]\n",
      "Epoch 45/100 (Loss - Train: 6.78e-04, Best val: 5.99e-04): 100%|██████████| 1109/1109 [00:03<00:00, 327.94it/s]\n",
      "Epoch 46/100 (Loss - Train: 6.82e-04, Best val: 5.99e-04): 100%|██████████| 1109/1109 [00:03<00:00, 319.36it/s]\n",
      "Epoch 47/100 (Loss - Train: 6.72e-04, Best val: 5.99e-04): 100%|██████████| 1109/1109 [00:03<00:00, 345.36it/s]\n",
      "Epoch 48/100 (Loss - Train: 6.62e-04, Best val: 5.99e-04): 100%|██████████| 1109/1109 [00:03<00:00, 314.30it/s]\n",
      "Epoch 49/100 (Loss - Train: 6.81e-04, Best val: 5.99e-04): 100%|██████████| 1109/1109 [00:03<00:00, 317.88it/s]\n",
      "Epoch 50/100 (Loss - Train: 6.71e-04, Best val: 5.99e-04): 100%|██████████| 1109/1109 [00:03<00:00, 277.55it/s]\n",
      "Epoch 51/100 (Loss - Train: 6.79e-04, Best val: 5.99e-04): 100%|██████████| 1109/1109 [00:03<00:00, 319.85it/s]\n",
      "Epoch 52/100 (Loss - Train: 6.78e-04, Best val: 5.99e-04): 100%|██████████| 1109/1109 [00:03<00:00, 299.74it/s]\n",
      "Epoch 53/100 (Loss - Train: 6.67e-04, Best val: 5.79e-04): 100%|██████████| 1109/1109 [00:04<00:00, 276.44it/s]\n",
      "Epoch 54/100 (Loss - Train: 6.73e-04, Best val: 5.79e-04): 100%|██████████| 1109/1109 [00:04<00:00, 243.74it/s]\n",
      "Epoch 55/100 (Loss - Train: 6.67e-04, Best val: 5.79e-04): 100%|██████████| 1109/1109 [00:04<00:00, 240.55it/s]\n",
      "Epoch 56/100 (Loss - Train: 6.75e-04, Best val: 5.79e-04): 100%|██████████| 1109/1109 [00:04<00:00, 237.04it/s]\n",
      "Epoch 57/100 (Loss - Train: 6.67e-04, Best val: 5.79e-04): 100%|██████████| 1109/1109 [00:04<00:00, 263.32it/s]\n",
      "Epoch 58/100 (Loss - Train: 6.77e-04, Best val: 5.79e-04): 100%|██████████| 1109/1109 [00:04<00:00, 255.97it/s]\n",
      "Epoch 59/100 (Loss - Train: 6.64e-04, Best val: 5.79e-04): 100%|██████████| 1109/1109 [00:04<00:00, 248.65it/s]\n",
      "Epoch 60/100 (Loss - Train: 6.72e-04, Best val: 5.79e-04): 100%|██████████| 1109/1109 [00:03<00:00, 279.95it/s]\n",
      "Epoch 61/100 (Loss - Train: 6.64e-04, Best val: 5.79e-04): 100%|██████████| 1109/1109 [00:04<00:00, 260.94it/s]\n",
      "Epoch 62/100 (Loss - Train: 6.64e-04, Best val: 5.79e-04): 100%|██████████| 1109/1109 [00:04<00:00, 264.57it/s]\n",
      "Epoch 63/100 (Loss - Train: 6.68e-04, Best val: 5.79e-04): 100%|██████████| 1109/1109 [00:04<00:00, 241.01it/s]\n",
      "Epoch 64/100 (Loss - Train: 6.64e-04, Best val: 5.79e-04): 100%|██████████| 1109/1109 [00:04<00:00, 263.61it/s]\n",
      "Epoch 65/100 (Loss - Train: 6.57e-04, Best val: 5.79e-04): 100%|██████████| 1109/1109 [00:04<00:00, 252.03it/s]\n",
      "Epoch 66/100 (Loss - Train: 6.71e-04, Best val: 5.79e-04): 100%|██████████| 1109/1109 [00:04<00:00, 260.78it/s]\n",
      "Epoch 67/100 (Loss - Train: 6.55e-04, Best val: 5.79e-04): 100%|██████████| 1109/1109 [00:04<00:00, 233.90it/s]\n",
      "Epoch 68/100 (Loss - Train: 6.79e-04, Best val: 5.79e-04): 100%|██████████| 1109/1109 [00:04<00:00, 239.34it/s]\n",
      "Epoch 69/100 (Loss - Train: 6.73e-04, Best val: 5.79e-04): 100%|██████████| 1109/1109 [00:03<00:00, 329.94it/s]\n",
      "Epoch 70/100 (Loss - Train: 6.70e-04, Best val: 5.78e-04): 100%|██████████| 1109/1109 [00:03<00:00, 323.30it/s]\n",
      "Epoch 71/100 (Loss - Train: 6.75e-04, Best val: 5.78e-04): 100%|██████████| 1109/1109 [00:03<00:00, 346.11it/s]\n",
      "Epoch 72/100 (Loss - Train: 6.72e-04, Best val: 5.78e-04): 100%|██████████| 1109/1109 [00:03<00:00, 323.32it/s]\n",
      "Epoch 73/100 (Loss - Train: 6.60e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:03<00:00, 336.78it/s]\n",
      "Epoch 74/100 (Loss - Train: 6.58e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:03<00:00, 322.15it/s]\n",
      "Epoch 75/100 (Loss - Train: 6.70e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 265.23it/s]\n",
      "Epoch 76/100 (Loss - Train: 6.65e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 263.82it/s]\n",
      "Epoch 77/100 (Loss - Train: 6.61e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 265.31it/s]\n",
      "Epoch 78/100 (Loss - Train: 6.73e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:03<00:00, 281.96it/s]\n",
      "Epoch 79/100 (Loss - Train: 6.65e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 274.96it/s]\n",
      "Epoch 80/100 (Loss - Train: 6.69e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 272.53it/s]\n",
      "Epoch 81/100 (Loss - Train: 6.58e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 261.76it/s]\n",
      "Epoch 82/100 (Loss - Train: 6.73e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 272.84it/s]\n",
      "Epoch 83/100 (Loss - Train: 6.57e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:03<00:00, 282.21it/s]\n",
      "Epoch 84/100 (Loss - Train: 6.64e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 275.42it/s]\n",
      "Epoch 85/100 (Loss - Train: 6.59e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 263.67it/s]\n",
      "Epoch 86/100 (Loss - Train: 6.65e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 277.16it/s]\n",
      "Epoch 87/100 (Loss - Train: 6.73e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:03<00:00, 277.85it/s]\n",
      "Epoch 88/100 (Loss - Train: 6.62e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 271.76it/s]\n",
      "Epoch 89/100 (Loss - Train: 6.67e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:03<00:00, 279.34it/s]\n",
      "Epoch 90/100 (Loss - Train: 6.64e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 252.06it/s]\n",
      "Epoch 91/100 (Loss - Train: 6.60e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 265.20it/s]\n",
      "Epoch 92/100 (Loss - Train: 6.67e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 273.22it/s]\n",
      "Epoch 93/100 (Loss - Train: 6.81e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 245.97it/s]\n",
      "Epoch 94/100 (Loss - Train: 6.65e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 252.21it/s]\n",
      "Epoch 95/100 (Loss - Train: 6.67e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 245.91it/s]\n",
      "Epoch 96/100 (Loss - Train: 6.74e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:05<00:00, 219.72it/s]\n",
      "Epoch 97/100 (Loss - Train: 6.71e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 262.00it/s]\n",
      "Epoch 98/100 (Loss - Train: 6.75e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 252.02it/s]\n",
      "Epoch 99/100 (Loss - Train: 6.67e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 270.66it/s]\n",
      "Epoch 100/100 (Loss - Train: 6.82e-04, Best val: 5.73e-04): 100%|██████████| 1109/1109 [00:04<00:00, 271.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Train loss  \\\n",
      "Model          Dataset             Timestamp                     \n",
      "BBM1_SimpleNet MinMaxNormalizedOPF 20240228-144804    0.000698   \n",
      "\n",
      "                                                    Validation loss  \\\n",
      "Model          Dataset             Timestamp                          \n",
      "BBM1_SimpleNet MinMaxNormalizedOPF 20240228-144804         0.000677   \n",
      "\n",
      "                                                    Test loss  Input size  \\\n",
      "Model          Dataset             Timestamp                                \n",
      "BBM1_SimpleNet MinMaxNormalizedOPF 20240228-144804   0.000667          51   \n",
      "\n",
      "                                                    Output size  \\\n",
      "Model          Dataset             Timestamp                      \n",
      "BBM1_SimpleNet MinMaxNormalizedOPF 20240228-144804            9   \n",
      "\n",
      "                                                    Training time [ms]  \\\n",
      "Model          Dataset             Timestamp                             \n",
      "BBM1_SimpleNet MinMaxNormalizedOPF 20240228-144804          464667.564   \n",
      "\n",
      "                                                                                           Model path  \n",
      "Model          Dataset             Timestamp                                                           \n",
      "BBM1_SimpleNet MinMaxNormalizedOPF 20240228-144804  models\\BBM1_SimpleNet_MinMaxNormalizedOPF_2024...  \n",
      "------ Finished! ------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\projects\\IPTLC_BBMs\\src\\greyboxmodels\\bbmcpsmodels\\creator.py:413: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  comparison = (model_name, dataset_name, timestamp) in results_table.index\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "BBM_creator.train(save_to=\"models\", epochs=100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T13:55:56.706273300Z",
     "start_time": "2024-02-28T13:48:04.263962700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    Train loss  \\\nModel          Dataset             Timestamp                     \nBBM1_SimpleNet MinMaxNormalizedOPF 20240228-144804    0.000694   \n\n                                                    Validation loss  \\\nModel          Dataset             Timestamp                          \nBBM1_SimpleNet MinMaxNormalizedOPF 20240228-144804         0.000679   \n\n                                                    Test loss  Input size  \\\nModel          Dataset             Timestamp                                \nBBM1_SimpleNet MinMaxNormalizedOPF 20240228-144804   0.000671          51   \n\n                                                    Output size  \\\nModel          Dataset             Timestamp                      \nBBM1_SimpleNet MinMaxNormalizedOPF 20240228-144804            9   \n\n                                                    Training time [ms]  \\\nModel          Dataset             Timestamp                             \nBBM1_SimpleNet MinMaxNormalizedOPF 20240228-144804          464667.564   \n\n                                                                                           Model path  \nModel          Dataset             Timestamp                                                           \nBBM1_SimpleNet MinMaxNormalizedOPF 20240228-144804  models\\BBM1_SimpleNet_MinMaxNormalizedOPF_2024...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>Train loss</th>\n      <th>Validation loss</th>\n      <th>Test loss</th>\n      <th>Input size</th>\n      <th>Output size</th>\n      <th>Training time [ms]</th>\n      <th>Model path</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th>Dataset</th>\n      <th>Timestamp</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>BBM1_SimpleNet</th>\n      <th>MinMaxNormalizedOPF</th>\n      <th>20240228-144804</th>\n      <td>0.000694</td>\n      <td>0.000679</td>\n      <td>0.000671</td>\n      <td>51</td>\n      <td>9</td>\n      <td>464667.564</td>\n      <td>models\\BBM1_SimpleNet_MinMaxNormalizedOPF_2024...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BBM_creator._summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T14:18:42.611581Z",
     "start_time": "2024-02-28T14:18:39.563108200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BBM 2: a two-layer feedforward neural network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "input_size = loaders[0].dataset[0][0].shape[0]\n",
    "output_size = loaders[0].dataset[0][1].shape[0]\n",
    "\n",
    "BBM_creator.instantiate_model(opf_bbm.BBM2_DeepNN, input_size, output_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T09:33:31.554895200Z",
     "start_time": "2024-02-29T09:33:31.374712900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Training model 'BBM2-deep' ------\n",
      "Models and summary will be save to 'models/' (will be created if it does not exist)\n",
      "    - Model path: models\\BBM2-deep_MinMaxNormalizedOPF_20240229-103342.pt\n",
      "    - Summary path: models/models_summary.csv\n",
      "Training on cuda:0\n",
      "Training starts in:  00:00\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 (Loss - Train: 4.56e-02, Best val: N/A): 100%|██████████| 177/177 [00:03<00:00, 52.11it/s] \n",
      "Epoch 2/100 (Loss - Train: 1.34e-02, Best val: 0.02): 100%|██████████| 177/177 [00:00<00:00, 260.37it/s]\n",
      "Epoch 3/100 (Loss - Train: 1.00e-02, Best val: 0.01): 100%|██████████| 177/177 [00:00<00:00, 226.59it/s]\n",
      "Epoch 4/100 (Loss - Train: 7.00e-03, Best val: 7.57e-03): 100%|██████████| 177/177 [00:00<00:00, 226.88it/s]\n",
      "Epoch 5/100 (Loss - Train: 5.28e-03, Best val: 5.70e-03): 100%|██████████| 177/177 [00:00<00:00, 258.41it/s]\n",
      "Epoch 6/100 (Loss - Train: 3.81e-03, Best val: 3.84e-03): 100%|██████████| 177/177 [00:00<00:00, 245.81it/s]\n",
      "Epoch 7/100 (Loss - Train: 3.39e-03, Best val: 3.36e-03): 100%|██████████| 177/177 [00:00<00:00, 219.87it/s]\n",
      "Epoch 8/100 (Loss - Train: 3.07e-03, Best val: 3.20e-03): 100%|██████████| 177/177 [00:00<00:00, 234.43it/s]\n",
      "Epoch 9/100 (Loss - Train: 3.20e-03, Best val: 2.97e-03): 100%|██████████| 177/177 [00:00<00:00, 252.86it/s]\n",
      "Epoch 10/100 (Loss - Train: 2.94e-03, Best val: 2.88e-03): 100%|██████████| 177/177 [00:00<00:00, 208.93it/s]\n",
      "Epoch 11/100 (Loss - Train: 2.77e-03, Best val: 2.84e-03): 100%|██████████| 177/177 [00:00<00:00, 219.35it/s]\n",
      "Epoch 12/100 (Loss - Train: 2.64e-03, Best val: 2.45e-03): 100%|██████████| 177/177 [00:00<00:00, 268.13it/s]\n",
      "Epoch 13/100 (Loss - Train: 2.60e-03, Best val: 2.38e-03): 100%|██████████| 177/177 [00:00<00:00, 214.89it/s]\n",
      "Epoch 14/100 (Loss - Train: 2.54e-03, Best val: 2.38e-03): 100%|██████████| 177/177 [00:00<00:00, 215.19it/s]\n",
      "Epoch 15/100 (Loss - Train: 2.42e-03, Best val: 2.12e-03): 100%|██████████| 177/177 [00:00<00:00, 222.57it/s]\n",
      "Epoch 16/100 (Loss - Train: 2.36e-03, Best val: 2.12e-03): 100%|██████████| 177/177 [00:00<00:00, 228.31it/s]\n",
      "Epoch 17/100 (Loss - Train: 2.17e-03, Best val: 2.00e-03): 100%|██████████| 177/177 [00:00<00:00, 238.19it/s]\n",
      "Epoch 18/100 (Loss - Train: 2.22e-03, Best val: 1.87e-03): 100%|██████████| 177/177 [00:00<00:00, 221.52it/s]\n",
      "Epoch 19/100 (Loss - Train: 2.22e-03, Best val: 1.87e-03): 100%|██████████| 177/177 [00:00<00:00, 216.30it/s]\n",
      "Epoch 20/100 (Loss - Train: 2.21e-03, Best val: 1.87e-03): 100%|██████████| 177/177 [00:00<00:00, 233.38it/s]\n",
      "Epoch 21/100 (Loss - Train: 2.03e-03, Best val: 1.87e-03): 100%|██████████| 177/177 [00:00<00:00, 260.27it/s]\n",
      "Epoch 22/100 (Loss - Train: 1.98e-03, Best val: 1.87e-03): 100%|██████████| 177/177 [00:00<00:00, 273.20it/s]\n",
      "Epoch 23/100 (Loss - Train: 1.82e-03, Best val: 1.87e-03): 100%|██████████| 177/177 [00:00<00:00, 232.84it/s]\n",
      "Epoch 24/100 (Loss - Train: 1.83e-03, Best val: 1.86e-03): 100%|██████████| 177/177 [00:00<00:00, 223.19it/s]\n",
      "Epoch 25/100 (Loss - Train: 1.78e-03, Best val: 1.61e-03): 100%|██████████| 177/177 [00:00<00:00, 280.92it/s]\n",
      "Epoch 26/100 (Loss - Train: 1.75e-03, Best val: 1.61e-03): 100%|██████████| 177/177 [00:00<00:00, 232.88it/s]\n",
      "Epoch 27/100 (Loss - Train: 1.80e-03, Best val: 1.61e-03): 100%|██████████| 177/177 [00:00<00:00, 215.53it/s]\n",
      "Epoch 28/100 (Loss - Train: 1.77e-03, Best val: 1.61e-03): 100%|██████████| 177/177 [00:00<00:00, 249.57it/s]\n",
      "Epoch 29/100 (Loss - Train: 1.75e-03, Best val: 1.61e-03): 100%|██████████| 177/177 [00:00<00:00, 236.13it/s]\n",
      "Epoch 30/100 (Loss - Train: 1.66e-03, Best val: 1.61e-03): 100%|██████████| 177/177 [00:00<00:00, 226.97it/s]\n",
      "Epoch 31/100 (Loss - Train: 1.74e-03, Best val: 1.56e-03): 100%|██████████| 177/177 [00:00<00:00, 216.19it/s]\n",
      "Epoch 32/100 (Loss - Train: 1.84e-03, Best val: 1.56e-03): 100%|██████████| 177/177 [00:00<00:00, 235.74it/s]\n",
      "Epoch 33/100 (Loss - Train: 1.89e-03, Best val: 1.56e-03): 100%|██████████| 177/177 [00:00<00:00, 215.88it/s]\n",
      "Epoch 34/100 (Loss - Train: 1.85e-03, Best val: 1.56e-03): 100%|██████████| 177/177 [00:00<00:00, 212.49it/s]\n",
      "Epoch 35/100 (Loss - Train: 1.64e-03, Best val: 1.56e-03): 100%|██████████| 177/177 [00:00<00:00, 238.35it/s]\n",
      "Epoch 36/100 (Loss - Train: 1.53e-03, Best val: 1.56e-03): 100%|██████████| 177/177 [00:00<00:00, 215.26it/s]\n",
      "Epoch 37/100 (Loss - Train: 1.66e-03, Best val: 1.56e-03): 100%|██████████| 177/177 [00:00<00:00, 240.15it/s]\n",
      "Epoch 38/100 (Loss - Train: 1.55e-03, Best val: 1.56e-03): 100%|██████████| 177/177 [00:00<00:00, 208.27it/s]\n",
      "Epoch 39/100 (Loss - Train: 1.58e-03, Best val: 1.56e-03): 100%|██████████| 177/177 [00:00<00:00, 181.07it/s]\n",
      "Epoch 40/100 (Loss - Train: 1.55e-03, Best val: 1.56e-03): 100%|██████████| 177/177 [00:00<00:00, 198.91it/s]\n",
      "Epoch 41/100 (Loss - Train: 1.63e-03, Best val: 1.56e-03): 100%|██████████| 177/177 [00:00<00:00, 225.95it/s]\n",
      "Epoch 42/100 (Loss - Train: 1.60e-03, Best val: 1.56e-03): 100%|██████████| 177/177 [00:00<00:00, 182.42it/s]\n",
      "Epoch 43/100 (Loss - Train: 1.49e-03, Best val: 1.54e-03): 100%|██████████| 177/177 [00:00<00:00, 215.36it/s]\n",
      "Epoch 44/100 (Loss - Train: 1.52e-03, Best val: 1.41e-03): 100%|██████████| 177/177 [00:00<00:00, 228.56it/s]\n",
      "Epoch 45/100 (Loss - Train: 1.52e-03, Best val: 1.41e-03): 100%|██████████| 177/177 [00:00<00:00, 196.64it/s]\n",
      "Epoch 46/100 (Loss - Train: 1.44e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:00<00:00, 196.64it/s]\n",
      "Epoch 47/100 (Loss - Train: 1.39e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:00<00:00, 194.47it/s]\n",
      "Epoch 48/100 (Loss - Train: 1.46e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:01<00:00, 167.25it/s]\n",
      "Epoch 49/100 (Loss - Train: 1.54e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:01<00:00, 176.95it/s]\n",
      "Epoch 50/100 (Loss - Train: 1.35e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:00<00:00, 208.32it/s]\n",
      "Epoch 51/100 (Loss - Train: 1.45e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:00<00:00, 192.44it/s]\n",
      "Epoch 52/100 (Loss - Train: 1.41e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:00<00:00, 215.88it/s]\n",
      "Epoch 53/100 (Loss - Train: 1.58e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:00<00:00, 229.88it/s]\n",
      "Epoch 54/100 (Loss - Train: 1.38e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:00<00:00, 209.45it/s]\n",
      "Epoch 55/100 (Loss - Train: 1.62e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:00<00:00, 201.98it/s]\n",
      "Epoch 56/100 (Loss - Train: 1.32e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:00<00:00, 227.89it/s]\n",
      "Epoch 57/100 (Loss - Train: 1.35e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:00<00:00, 221.22it/s]\n",
      "Epoch 58/100 (Loss - Train: 1.45e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:00<00:00, 214.72it/s]\n",
      "Epoch 59/100 (Loss - Train: 1.45e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:00<00:00, 218.55it/s]\n",
      "Epoch 60/100 (Loss - Train: 1.43e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:01<00:00, 175.29it/s]\n",
      "Epoch 61/100 (Loss - Train: 1.33e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:01<00:00, 152.57it/s]\n",
      "Epoch 62/100 (Loss - Train: 1.53e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:00<00:00, 192.38it/s]\n",
      "Epoch 63/100 (Loss - Train: 1.46e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:00<00:00, 197.92it/s]\n",
      "Epoch 64/100 (Loss - Train: 1.50e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:00<00:00, 191.44it/s]\n",
      "Epoch 65/100 (Loss - Train: 1.43e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:00<00:00, 222.21it/s]\n",
      "Epoch 66/100 (Loss - Train: 1.41e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:00<00:00, 203.38it/s]\n",
      "Epoch 67/100 (Loss - Train: 1.47e-03, Best val: 1.37e-03): 100%|██████████| 177/177 [00:00<00:00, 188.57it/s]\n",
      "Epoch 68/100 (Loss - Train: 1.43e-03, Best val: 1.31e-03): 100%|██████████| 177/177 [00:00<00:00, 208.31it/s]\n",
      "Epoch 69/100 (Loss - Train: 1.37e-03, Best val: 1.31e-03): 100%|██████████| 177/177 [00:01<00:00, 172.64it/s]\n",
      "Epoch 70/100 (Loss - Train: 1.37e-03, Best val: 1.31e-03): 100%|██████████| 177/177 [00:00<00:00, 198.90it/s]\n",
      "Epoch 71/100 (Loss - Train: 1.21e-03, Best val: 1.30e-03): 100%|██████████| 177/177 [00:00<00:00, 210.69it/s]\n",
      "Epoch 72/100 (Loss - Train: 1.43e-03, Best val: 1.30e-03): 100%|██████████| 177/177 [00:00<00:00, 193.73it/s]\n",
      "Epoch 73/100 (Loss - Train: 1.33e-03, Best val: 1.30e-03): 100%|██████████| 177/177 [00:00<00:00, 190.28it/s]\n",
      "Epoch 74/100 (Loss - Train: 1.34e-03, Best val: 1.30e-03): 100%|██████████| 177/177 [00:00<00:00, 190.28it/s]\n",
      "Epoch 75/100 (Loss - Train: 1.44e-03, Best val: 1.30e-03): 100%|██████████| 177/177 [00:00<00:00, 199.92it/s]\n",
      "Epoch 76/100 (Loss - Train: 1.35e-03, Best val: 1.30e-03): 100%|██████████| 177/177 [00:00<00:00, 198.57it/s]\n",
      "Epoch 77/100 (Loss - Train: 1.51e-03, Best val: 1.30e-03): 100%|██████████| 177/177 [00:01<00:00, 173.49it/s]\n",
      "Epoch 78/100 (Loss - Train: 1.30e-03, Best val: 1.30e-03): 100%|██████████| 177/177 [00:00<00:00, 186.96it/s]\n",
      "Epoch 79/100 (Loss - Train: 1.29e-03, Best val: 1.30e-03): 100%|██████████| 177/177 [00:00<00:00, 208.18it/s]\n",
      "Epoch 80/100 (Loss - Train: 1.36e-03, Best val: 1.30e-03): 100%|██████████| 177/177 [00:00<00:00, 184.30it/s]\n",
      "Epoch 81/100 (Loss - Train: 1.29e-03, Best val: 1.30e-03): 100%|██████████| 177/177 [00:00<00:00, 190.34it/s]\n",
      "Epoch 82/100 (Loss - Train: 1.18e-03, Best val: 1.30e-03): 100%|██████████| 177/177 [00:00<00:00, 186.66it/s]\n",
      "Epoch 83/100 (Loss - Train: 1.22e-03, Best val: 1.30e-03): 100%|██████████| 177/177 [00:01<00:00, 162.88it/s]\n",
      "Epoch 84/100 (Loss - Train: 1.23e-03, Best val: 1.30e-03): 100%|██████████| 177/177 [00:00<00:00, 198.88it/s]\n",
      "Epoch 85/100 (Loss - Train: 1.28e-03, Best val: 1.30e-03): 100%|██████████| 177/177 [00:00<00:00, 203.51it/s]\n",
      "Epoch 86/100 (Loss - Train: 1.34e-03, Best val: 1.30e-03): 100%|██████████| 177/177 [00:00<00:00, 190.96it/s]\n",
      "Epoch 87/100 (Loss - Train: 1.35e-03, Best val: 1.30e-03): 100%|██████████| 177/177 [00:00<00:00, 203.44it/s]\n",
      "Epoch 88/100 (Loss - Train: 1.22e-03, Best val: 1.16e-03): 100%|██████████| 177/177 [00:00<00:00, 186.20it/s]\n",
      "Epoch 89/100 (Loss - Train: 1.28e-03, Best val: 1.16e-03): 100%|██████████| 177/177 [00:01<00:00, 137.21it/s]\n",
      "Epoch 90/100 (Loss - Train: 1.19e-03, Best val: 1.16e-03): 100%|██████████| 177/177 [00:00<00:00, 197.30it/s]\n",
      "Epoch 91/100 (Loss - Train: 1.24e-03, Best val: 1.16e-03): 100%|██████████| 177/177 [00:00<00:00, 192.36it/s]\n",
      "Epoch 92/100 (Loss - Train: 1.23e-03, Best val: 1.16e-03): 100%|██████████| 177/177 [00:00<00:00, 181.89it/s]\n",
      "Epoch 93/100 (Loss - Train: 1.24e-03, Best val: 1.16e-03): 100%|██████████| 177/177 [00:00<00:00, 210.11it/s]\n",
      "Epoch 94/100 (Loss - Train: 1.22e-03, Best val: 1.16e-03): 100%|██████████| 177/177 [00:00<00:00, 180.14it/s]\n",
      "Epoch 95/100 (Loss - Train: 1.25e-03, Best val: 1.16e-03): 100%|██████████| 177/177 [00:00<00:00, 203.97it/s]\n",
      "Epoch 96/100 (Loss - Train: 1.19e-03, Best val: 1.16e-03): 100%|██████████| 177/177 [00:00<00:00, 235.70it/s]\n",
      "Epoch 97/100 (Loss - Train: 1.10e-03, Best val: 1.16e-03): 100%|██████████| 177/177 [00:00<00:00, 194.51it/s]\n",
      "Epoch 98/100 (Loss - Train: 1.21e-03, Best val: 1.16e-03): 100%|██████████| 177/177 [00:00<00:00, 185.94it/s]\n",
      "Epoch 99/100 (Loss - Train: 1.16e-03, Best val: 1.16e-03): 100%|██████████| 177/177 [00:00<00:00, 194.66it/s]\n",
      "Epoch 100/100 (Loss - Train: 1.25e-03, Best val: 1.16e-03): 100%|██████████| 177/177 [00:00<00:00, 197.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Train loss  Validation loss  \\\n",
      "Model     Dataset             Timestamp                                      \n",
      "BBM2-deep MinMaxNormalizedOPF 20240229-103342    0.001148         0.001237   \n",
      "\n",
      "                                               Test loss  Input size  \\\n",
      "Model     Dataset             Timestamp                                \n",
      "BBM2-deep MinMaxNormalizedOPF 20240229-103342   0.001086          51   \n",
      "\n",
      "                                               Output size  \\\n",
      "Model     Dataset             Timestamp                      \n",
      "BBM2-deep MinMaxNormalizedOPF 20240229-103342            9   \n",
      "\n",
      "                                               Training time [ms]  \\\n",
      "Model     Dataset             Timestamp                             \n",
      "BBM2-deep MinMaxNormalizedOPF 20240229-103342           96384.134   \n",
      "\n",
      "                                                                                      Model path  \n",
      "Model     Dataset             Timestamp                                                           \n",
      "BBM2-deep MinMaxNormalizedOPF 20240229-103342  models\\BBM2-deep_MinMaxNormalizedOPF_20240229-...  \n",
      "------ Finished! ------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\projects\\IPTLC_BBMs\\src\\greyboxmodels\\bbmcpsmodels\\creator.py:413: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  comparison = (model_name, dataset_name, timestamp) in results_table.index\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "BBM_creator.train(save_to=\"models\", epochs=100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T09:35:23.714384500Z",
     "start_time": "2024-02-29T09:33:42.574888800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               Train loss  Validation loss  \\\nModel     Dataset             Timestamp                                      \nBBM2-deep MinMaxNormalizedOPF 20240229-103342    0.001178         0.001285   \n\n                                               Test loss  Input size  \\\nModel     Dataset             Timestamp                                \nBBM2-deep MinMaxNormalizedOPF 20240229-103342   0.001044          51   \n\n                                               Output size  \\\nModel     Dataset             Timestamp                      \nBBM2-deep MinMaxNormalizedOPF 20240229-103342            9   \n\n                                               Training time [ms]  \\\nModel     Dataset             Timestamp                             \nBBM2-deep MinMaxNormalizedOPF 20240229-103342           96384.134   \n\n                                                                                      Model path  \nModel     Dataset             Timestamp                                                           \nBBM2-deep MinMaxNormalizedOPF 20240229-103342  models\\BBM2-deep_MinMaxNormalizedOPF_20240229-...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>Train loss</th>\n      <th>Validation loss</th>\n      <th>Test loss</th>\n      <th>Input size</th>\n      <th>Output size</th>\n      <th>Training time [ms]</th>\n      <th>Model path</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th>Dataset</th>\n      <th>Timestamp</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>BBM2-deep</th>\n      <th>MinMaxNormalizedOPF</th>\n      <th>20240229-103342</th>\n      <td>0.001178</td>\n      <td>0.001285</td>\n      <td>0.001044</td>\n      <td>51</td>\n      <td>9</td>\n      <td>96384.134</td>\n      <td>models\\BBM2-deep_MinMaxNormalizedOPF_20240229-...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BBM_creator._summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T09:40:00.273685800Z",
     "start_time": "2024-02-29T09:39:59.755560800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LEGACY CODE BELOW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BBM 1: a two-layer feedforward neural network"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T13:01:34.548144500Z",
     "start_time": "2024-01-30T13:01:34.541667300Z"
    }
   },
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalized dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: data\\OPF\\2023-12-06_18-00-46\\OPF-NI1-O3\n",
      "---- Dataset loaded ----\n",
      "Input shape: (59620, 53)\n",
      "Output shape: (59620, 9)\n",
      "---- Removing NaNs ----\n",
      "Rows to delete: [57886 57887 57888 57889 57890 57891 57892 57893 57894 57895 57896 57897\n",
      " 57898 57899 57900 57901 57902 57903 57904 57905 57906 57907 57908 57909\n",
      " 57910 57911 57912 57913 57914 57915 57916 57917 57918 57919 57920 57921\n",
      " 57922 57923 57924 57925 57926 57927 57928 57929 57930]\n",
      "Input shape: (59575, 53)\n",
      "Output shape: (59575, 9)\n",
      "---- Converting to torch tensors ----\n",
      "---- Dataset loaded! ----\n"
     ]
    }
   ],
   "source": [
    "# Set up the dataset\n",
    "dataset_name = \"OPF-NI1-O3\"\n",
    "loaders = creator.setup_datasets(datasets_folder, dataset_name, remove_nans=True, ratios=(0.70, 0.15, 0.15), batch_size=32)\n",
    "BBM_creator.set_dataloaders(*loaders)\n",
    "\n",
    "# Instantiate model\n",
    "input_size = loaders[0].dataset[0][0].shape[0]\n",
    "output_size = loaders[0].dataset[0][1].shape[0]\n",
    "BBM_creator.instantiate_model(opf_bbm.BBM1_SimpleNet, input_size, output_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:09:21.838820900Z",
     "start_time": "2024-02-01T16:09:21.761928500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Training model 'BBM1_SimpleNet' ------\n",
      "Models and summary will be save to 'models/' (will be created if it does not exist)\n",
      "    - Model path: models\\BBM1_SimpleNet_OPF-NI1-O3_20240201-170922.pt\n",
      "    - Summary path: models/models_summary.csv\n",
      "Training on cuda:0\n",
      "Training starts in:  00:00\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 (Loss - Train: 2.44e+02, Best val: N/A): 100%|██████████| 1304/1304 [00:04<00:00, 314.37it/s]\n",
      "Epoch 2/100 (Loss - Train: 3.94e+01, Best val: 44.63): 100%|██████████| 1304/1304 [00:04<00:00, 317.37it/s]\n",
      "Epoch 3/100 (Loss - Train: 3.79e+01, Best val: 43.36): 100%|██████████| 1304/1304 [00:04<00:00, 316.83it/s]\n",
      "Epoch 4/100 (Loss - Train: 3.58e+01, Best val: 41.88): 100%|██████████| 1304/1304 [00:04<00:00, 317.26it/s]\n",
      "Epoch 5/100 (Loss - Train: 3.32e+01, Best val: 38.12): 100%|██████████| 1304/1304 [00:04<00:00, 312.62it/s]\n",
      "Epoch 6/100 (Loss - Train: 3.06e+01, Best val: 34.91): 100%|██████████| 1304/1304 [00:04<00:00, 314.01it/s]\n",
      "Epoch 7/100 (Loss - Train: 2.81e+01, Best val: 30.88): 100%|██████████| 1304/1304 [00:04<00:00, 318.16it/s]\n",
      "Epoch 8/100 (Loss - Train: 2.64e+01, Best val: 27.82): 100%|██████████| 1304/1304 [00:04<00:00, 319.93it/s]\n",
      "Epoch 9/100 (Loss - Train: 2.53e+01, Best val: 26.34): 100%|██████████| 1304/1304 [00:04<00:00, 321.83it/s]\n",
      "Epoch 10/100 (Loss - Train: 2.47e+01, Best val: 24.76): 100%|██████████| 1304/1304 [00:04<00:00, 294.77it/s]\n",
      "Epoch 11/100 (Loss - Train: 2.43e+01, Best val: 24.16): 100%|██████████| 1304/1304 [00:04<00:00, 291.05it/s]\n",
      "Epoch 12/100 (Loss - Train: 2.39e+01, Best val: 23.56): 100%|██████████| 1304/1304 [00:04<00:00, 313.81it/s]\n",
      "Epoch 13/100 (Loss - Train: 2.36e+01, Best val: 23.30): 100%|██████████| 1304/1304 [00:04<00:00, 323.89it/s]\n",
      "Epoch 14/100 (Loss - Train: 2.34e+01, Best val: 22.94): 100%|██████████| 1304/1304 [00:04<00:00, 316.93it/s]\n",
      "Epoch 15/100 (Loss - Train: 2.33e+01, Best val: 22.94): 100%|██████████| 1304/1304 [00:04<00:00, 309.05it/s]\n",
      "Epoch 16/100 (Loss - Train: 2.32e+01, Best val: 22.63): 100%|██████████| 1304/1304 [00:05<00:00, 243.40it/s]\n",
      "Epoch 17/100 (Loss - Train: 2.32e+01, Best val: 22.63): 100%|██████████| 1304/1304 [00:04<00:00, 269.10it/s]\n",
      "Epoch 18/100 (Loss - Train: 2.32e+01, Best val: 22.63): 100%|██████████| 1304/1304 [00:04<00:00, 288.84it/s]\n",
      "Epoch 19/100 (Loss - Train: 2.32e+01, Best val: 22.51): 100%|██████████| 1304/1304 [00:04<00:00, 313.25it/s]\n",
      "Epoch 20/100 (Loss - Train: 2.31e+01, Best val: 22.51): 100%|██████████| 1304/1304 [00:04<00:00, 286.82it/s]\n",
      "Epoch 21/100 (Loss - Train: 2.30e+01, Best val: 22.29): 100%|██████████| 1304/1304 [00:04<00:00, 310.91it/s]\n",
      "Epoch 22/100 (Loss - Train: 2.30e+01, Best val: 22.29): 100%|██████████| 1304/1304 [00:03<00:00, 326.38it/s]\n",
      "Epoch 23/100 (Loss - Train: 2.30e+01, Best val: 22.29): 100%|██████████| 1304/1304 [00:04<00:00, 305.70it/s]\n",
      "Epoch 24/100 (Loss - Train: 2.30e+01, Best val: 22.29): 100%|██████████| 1304/1304 [00:04<00:00, 305.96it/s]\n",
      "Epoch 25/100 (Loss - Train: 2.30e+01, Best val: 22.29): 100%|██████████| 1304/1304 [00:04<00:00, 310.99it/s]\n",
      "Epoch 26/100 (Loss - Train: 2.29e+01, Best val: 22.29): 100%|██████████| 1304/1304 [00:04<00:00, 286.59it/s]\n",
      "Epoch 27/100 (Loss - Train: 2.29e+01, Best val: 22.29): 100%|██████████| 1304/1304 [00:04<00:00, 312.34it/s]\n",
      "Epoch 28/100 (Loss - Train: 2.28e+01, Best val: 22.29): 100%|██████████| 1304/1304 [00:04<00:00, 323.39it/s]\n",
      "Epoch 29/100 (Loss - Train: 2.29e+01, Best val: 22.29): 100%|██████████| 1304/1304 [00:04<00:00, 304.49it/s]\n",
      "Epoch 30/100 (Loss - Train: 2.26e+01, Best val: 22.29): 100%|██████████| 1304/1304 [00:04<00:00, 312.61it/s]\n",
      "Epoch 31/100 (Loss - Train: 2.29e+01, Best val: 22.29): 100%|██████████| 1304/1304 [00:04<00:00, 320.86it/s]\n",
      "Epoch 32/100 (Loss - Train: 2.29e+01, Best val: 22.29): 100%|██████████| 1304/1304 [00:04<00:00, 301.80it/s]\n",
      "Epoch 33/100 (Loss - Train: 2.27e+01, Best val: 22.29): 100%|██████████| 1304/1304 [00:04<00:00, 318.85it/s]\n",
      "Epoch 34/100 (Loss - Train: 2.28e+01, Best val: 22.29): 100%|██████████| 1304/1304 [00:04<00:00, 310.22it/s]\n",
      "Epoch 35/100 (Loss - Train: 2.27e+01, Best val: 22.29): 100%|██████████| 1304/1304 [00:04<00:00, 299.67it/s]\n",
      "Epoch 36/100 (Loss - Train: 2.28e+01, Best val: 22.11): 100%|██████████| 1304/1304 [00:05<00:00, 243.78it/s]\n",
      "Epoch 37/100 (Loss - Train: 2.28e+01, Best val: 22.11): 100%|██████████| 1304/1304 [00:04<00:00, 299.33it/s]\n",
      "Epoch 38/100 (Loss - Train: 2.29e+01, Best val: 22.11): 100%|██████████| 1304/1304 [00:04<00:00, 288.87it/s]\n",
      "Epoch 39/100 (Loss - Train: 2.27e+01, Best val: 21.99): 100%|██████████| 1304/1304 [00:04<00:00, 284.22it/s]\n",
      "Epoch 40/100 (Loss - Train: 2.27e+01, Best val: 21.99): 100%|██████████| 1304/1304 [00:04<00:00, 269.53it/s]\n",
      "Epoch 41/100 (Loss - Train: 2.26e+01, Best val: 21.99): 100%|██████████| 1304/1304 [00:04<00:00, 291.75it/s]\n",
      "Epoch 42/100 (Loss - Train: 2.27e+01, Best val: 21.99): 100%|██████████| 1304/1304 [00:04<00:00, 308.91it/s]\n",
      "Epoch 43/100 (Loss - Train: 2.27e+01, Best val: 21.99): 100%|██████████| 1304/1304 [00:04<00:00, 267.97it/s]\n",
      "Epoch 44/100 (Loss - Train: 2.26e+01, Best val: 21.99): 100%|██████████| 1304/1304 [00:04<00:00, 302.46it/s]\n",
      "Epoch 45/100 (Loss - Train: 2.26e+01, Best val: 21.99): 100%|██████████| 1304/1304 [00:04<00:00, 288.60it/s]\n",
      "Epoch 46/100 (Loss - Train: 2.26e+01, Best val: 21.99): 100%|██████████| 1304/1304 [00:04<00:00, 289.38it/s]\n",
      "Epoch 47/100 (Loss - Train: 2.26e+01, Best val: 21.99): 100%|██████████| 1304/1304 [00:04<00:00, 297.59it/s]\n",
      "Epoch 48/100 (Loss - Train: 2.26e+01, Best val: 21.99): 100%|██████████| 1304/1304 [00:04<00:00, 290.22it/s]\n",
      "Epoch 49/100 (Loss - Train: 2.26e+01, Best val: 21.99): 100%|██████████| 1304/1304 [00:04<00:00, 269.94it/s]\n",
      "Epoch 50/100 (Loss - Train: 2.26e+01, Best val: 21.99): 100%|██████████| 1304/1304 [00:04<00:00, 306.42it/s]\n",
      "Epoch 51/100 (Loss - Train: 2.26e+01, Best val: 21.93): 100%|██████████| 1304/1304 [00:03<00:00, 352.29it/s]\n",
      "Epoch 52/100 (Loss - Train: 2.26e+01, Best val: 21.93): 100%|██████████| 1304/1304 [00:04<00:00, 319.31it/s]\n",
      "Epoch 53/100 (Loss - Train: 2.25e+01, Best val: 21.93): 100%|██████████| 1304/1304 [00:03<00:00, 336.26it/s]\n",
      "Epoch 54/100 (Loss - Train: 2.25e+01, Best val: 21.93): 100%|██████████| 1304/1304 [00:03<00:00, 352.30it/s]\n",
      "Epoch 55/100 (Loss - Train: 2.25e+01, Best val: 21.93): 100%|██████████| 1304/1304 [00:03<00:00, 353.92it/s]\n",
      "Epoch 56/100 (Loss - Train: 2.25e+01, Best val: 21.93): 100%|██████████| 1304/1304 [00:03<00:00, 351.85it/s]\n",
      "Epoch 57/100 (Loss - Train: 2.25e+01, Best val: 21.93): 100%|██████████| 1304/1304 [00:03<00:00, 333.90it/s]\n",
      "Epoch 58/100 (Loss - Train: 2.24e+01, Best val: 21.78): 100%|██████████| 1304/1304 [00:04<00:00, 286.32it/s]\n",
      "Epoch 59/100 (Loss - Train: 2.24e+01, Best val: 21.78): 100%|██████████| 1304/1304 [00:04<00:00, 276.79it/s]\n",
      "Epoch 60/100 (Loss - Train: 2.25e+01, Best val: 21.78): 100%|██████████| 1304/1304 [00:04<00:00, 277.98it/s]\n",
      "Epoch 61/100 (Loss - Train: 2.25e+01, Best val: 21.70): 100%|██████████| 1304/1304 [00:04<00:00, 298.51it/s]\n",
      "Epoch 62/100 (Loss - Train: 2.24e+01, Best val: 21.70): 100%|██████████| 1304/1304 [00:04<00:00, 293.61it/s]\n",
      "Epoch 63/100 (Loss - Train: 2.24e+01, Best val: 21.70): 100%|██████████| 1304/1304 [00:04<00:00, 277.75it/s]\n",
      "Epoch 64/100 (Loss - Train: 2.24e+01, Best val: 21.70): 100%|██████████| 1304/1304 [00:04<00:00, 261.54it/s]\n",
      "Epoch 65/100 (Loss - Train: 2.23e+01, Best val: 21.70): 100%|██████████| 1304/1304 [00:04<00:00, 311.07it/s]\n",
      "Epoch 66/100 (Loss - Train: 2.24e+01, Best val: 21.70): 100%|██████████| 1304/1304 [00:04<00:00, 287.97it/s]\n",
      "Epoch 67/100 (Loss - Train: 2.24e+01, Best val: 21.70): 100%|██████████| 1304/1304 [00:04<00:00, 281.35it/s]\n",
      "Epoch 68/100 (Loss - Train: 2.24e+01, Best val: 21.70): 100%|██████████| 1304/1304 [00:04<00:00, 281.54it/s]\n",
      "Epoch 69/100 (Loss - Train: 2.23e+01, Best val: 21.70): 100%|██████████| 1304/1304 [00:04<00:00, 284.42it/s]\n",
      "Epoch 70/100 (Loss - Train: 2.24e+01, Best val: 21.70): 100%|██████████| 1304/1304 [00:04<00:00, 290.45it/s]\n",
      "Epoch 71/100 (Loss - Train: 2.23e+01, Best val: 21.70): 100%|██████████| 1304/1304 [00:04<00:00, 289.27it/s]\n",
      "Epoch 72/100 (Loss - Train: 2.24e+01, Best val: 21.70): 100%|██████████| 1304/1304 [00:04<00:00, 302.80it/s]\n",
      "Epoch 73/100 (Loss - Train: 2.23e+01, Best val: 21.70): 100%|██████████| 1304/1304 [00:04<00:00, 293.42it/s]\n",
      "Epoch 74/100 (Loss - Train: 2.23e+01, Best val: 21.70): 100%|██████████| 1304/1304 [00:04<00:00, 278.51it/s]\n",
      "Epoch 75/100 (Loss - Train: 2.23e+01, Best val: 21.70): 100%|██████████| 1304/1304 [00:04<00:00, 283.50it/s]\n",
      "Epoch 76/100 (Loss - Train: 2.23e+01, Best val: 21.62): 100%|██████████| 1304/1304 [00:04<00:00, 312.95it/s]\n",
      "Epoch 77/100 (Loss - Train: 2.23e+01, Best val: 21.62): 100%|██████████| 1304/1304 [00:04<00:00, 279.64it/s]\n",
      "Epoch 78/100 (Loss - Train: 2.24e+01, Best val: 21.62): 100%|██████████| 1304/1304 [00:04<00:00, 273.38it/s]\n",
      "Epoch 79/100 (Loss - Train: 2.23e+01, Best val: 21.56): 100%|██████████| 1304/1304 [00:04<00:00, 296.91it/s]\n",
      "Epoch 80/100 (Loss - Train: 2.23e+01, Best val: 21.56): 100%|██████████| 1304/1304 [00:04<00:00, 272.06it/s]\n",
      "Epoch 81/100 (Loss - Train: 2.22e+01, Best val: 21.56): 100%|██████████| 1304/1304 [00:04<00:00, 302.18it/s]\n",
      "Epoch 82/100 (Loss - Train: 2.22e+01, Best val: 21.56): 100%|██████████| 1304/1304 [00:04<00:00, 278.37it/s]\n",
      "Epoch 83/100 (Loss - Train: 2.23e+01, Best val: 21.56): 100%|██████████| 1304/1304 [00:04<00:00, 279.92it/s]\n",
      "Epoch 84/100 (Loss - Train: 2.22e+01, Best val: 21.56): 100%|██████████| 1304/1304 [00:04<00:00, 316.29it/s]\n",
      "Epoch 85/100 (Loss - Train: 2.21e+01, Best val: 21.56): 100%|██████████| 1304/1304 [00:04<00:00, 313.78it/s]\n",
      "Epoch 86/100 (Loss - Train: 2.22e+01, Best val: 21.56): 100%|██████████| 1304/1304 [00:04<00:00, 316.75it/s]\n",
      "Epoch 87/100 (Loss - Train: 2.23e+01, Best val: 21.56): 100%|██████████| 1304/1304 [00:04<00:00, 318.44it/s]\n",
      "Epoch 88/100 (Loss - Train: 2.23e+01, Best val: 21.56): 100%|██████████| 1304/1304 [00:04<00:00, 322.57it/s]\n",
      "Epoch 89/100 (Loss - Train: 2.22e+01, Best val: 21.56): 100%|██████████| 1304/1304 [00:04<00:00, 277.35it/s]\n",
      "Epoch   90/100:   5%|▌         | 68/1304 [00:00<00:04, 258.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mBBM_creator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43msave_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodels\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\projects\\IPTLC_BBMs\\src\\creator.py:284\u001B[0m, in \u001B[0;36mBBMCreator.train\u001B[1;34m(self, save_to, t_cowntdown, **kwargs)\u001B[0m\n\u001B[0;32m    281\u001B[0m \u001B[38;5;66;03m# Training loop\u001B[39;00m\n\u001B[0;32m    282\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch_number \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepochs):\n\u001B[0;32m    283\u001B[0m     \u001B[38;5;66;03m# Train the model for one epoch\u001B[39;00m\n\u001B[1;32m--> 284\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch_number\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;66;03m# Evaluate the model on the validation set\u001B[39;00m\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidation_dataloader \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\projects\\IPTLC_BBMs\\src\\creator.py:223\u001B[0m, in \u001B[0;36mBBMCreator.train_epoch\u001B[1;34m(self, epoch_number)\u001B[0m\n\u001B[0;32m    220\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m    222\u001B[0m \u001B[38;5;66;03m# Optimize\u001B[39;00m\n\u001B[1;32m--> 223\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    225\u001B[0m \u001B[38;5;66;03m# Add the loss to the running loss\u001B[39;00m\n\u001B[0;32m    226\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32mC:\\mambarfoge\\envs\\CPS_GBM_IPTLC\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    276\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    277\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    278\u001B[0m                                \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 280\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    281\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    283\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32mC:\\mambarfoge\\envs\\CPS_GBM_IPTLC\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     32\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 33\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     35\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(prev_grad)\n",
      "File \u001B[1;32mC:\\mambarfoge\\envs\\CPS_GBM_IPTLC\\lib\\site-packages\\torch\\optim\\adam.py:141\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    130\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[0;32m    133\u001B[0m         group,\n\u001B[0;32m    134\u001B[0m         params_with_grad,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    138\u001B[0m         max_exp_avg_sqs,\n\u001B[0;32m    139\u001B[0m         state_steps)\n\u001B[1;32m--> 141\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    142\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    143\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    144\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    145\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    146\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    148\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    149\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    150\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    152\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    153\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32mC:\\mambarfoge\\envs\\CPS_GBM_IPTLC\\lib\\site-packages\\torch\\optim\\adam.py:281\u001B[0m, in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    279\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[1;32m--> 281\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    282\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    283\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    284\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    285\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    286\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    287\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    288\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    293\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    294\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    295\u001B[0m \u001B[43m     \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    296\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    297\u001B[0m \u001B[43m     \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\mambarfoge\\envs\\CPS_GBM_IPTLC\\lib\\site-packages\\torch\\optim\\adam.py:439\u001B[0m, in \u001B[0;36m_multi_tensor_adam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[0;32m    436\u001B[0m params_ \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mview_as_real(x) \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mis_complex(x) \u001B[38;5;28;01melse\u001B[39;00m x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m device_params]\n\u001B[0;32m    438\u001B[0m \u001B[38;5;66;03m# update steps\u001B[39;00m\n\u001B[1;32m--> 439\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_foreach_add_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice_state_steps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    441\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m weight_decay \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    442\u001B[0m     device_grads \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39m_foreach_add(device_grads, device_params, alpha\u001B[38;5;241m=\u001B[39mweight_decay)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "BBM_creator.train(save_to=\"models\", epochs=100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T16:16:35.504985800Z",
     "start_time": "2024-02-01T16:09:22.170299100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "                                           Train loss  Validation loss  \\\nModel          Dataset    Timestamp                                      \nBBM1_SimpleNet OPF-NI1-O3 20240201-162924    31.81697        31.556109   \n\n                                           Test loss  Input size  Output size  \\\nModel          Dataset    Timestamp                                             \nBBM1_SimpleNet OPF-NI1-O3 20240201-162924  32.631261          53            9   \n\n                                           Training time [ms]  \\\nModel          Dataset    Timestamp                             \nBBM1_SimpleNet OPF-NI1-O3 20240201-162924            490856.2   \n\n                                                                                  Model path  \nModel          Dataset    Timestamp                                                           \nBBM1_SimpleNet OPF-NI1-O3 20240201-162924  models\\BBM1_SimpleNet_OPF-NI1-O3_20240201-1629...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>Train loss</th>\n      <th>Validation loss</th>\n      <th>Test loss</th>\n      <th>Input size</th>\n      <th>Output size</th>\n      <th>Training time [ms]</th>\n      <th>Model path</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th>Dataset</th>\n      <th>Timestamp</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>BBM1_SimpleNet</th>\n      <th>OPF-NI1-O3</th>\n      <th>20240201-162924</th>\n      <td>31.81697</td>\n      <td>31.556109</td>\n      <td>32.631261</td>\n      <td>53</td>\n      <td>9</td>\n      <td>490856.2</td>\n      <td>models\\BBM1_SimpleNet_OPF-NI1-O3_20240201-1629...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BBM_creator._summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:42:46.128539400Z",
     "start_time": "2024-02-01T15:42:43.106850100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Raw dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: data\\OPF\\2023-12-06_18-00-46\\OPF-I1-O3\n",
      "---- Dataset loaded ----\n",
      "Input shape: (59620, 53)\n",
      "Output shape: (59620, 9)\n",
      "---- Removing NaNs ----\n",
      "Rows to delete: [57886 57887 57888 57889 57890 57891 57892 57893 57894 57895 57896 57897\n",
      " 57898 57899 57900 57901 57902 57903 57904 57905 57906 57907 57908 57909\n",
      " 57910 57911 57912 57913 57914 57915 57916 57917 57918 57919 57920 57921\n",
      " 57922 57923 57924 57925 57926 57927 57928 57929 57930]\n",
      "Input shape: (59575, 53)\n",
      "Output shape: (59575, 9)\n",
      "---- Converting to torch tensors ----\n",
      "---- Dataset loaded! ----\n"
     ]
    }
   ],
   "source": [
    "# Set up the dataset\n",
    "dataset_name = \"OPF-I1-O3\"\n",
    "loaders = creator.setup_datasets(datasets_folder, dataset_name, remove_nans=True, ratios=(0.70, 0.15, 0.15), batch_size=32)\n",
    "BBM_creator.set_dataloaders(*loaders)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T10:13:17.908768200Z",
     "start_time": "2024-02-01T10:13:17.682312900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "input_size = loaders[0].dataset[0][0].shape[0]\n",
    "output_size = loaders[0].dataset[0][1].shape[0]\n",
    "BBM_creator.instantiate_model(opf_bbm.BBM1_SimpleNet, input_size, output_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T10:13:17.908768200Z",
     "start_time": "2024-02-01T10:13:17.789679700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Training model 'BBM1-2layers' ------\n",
      "Models and summary will be save to 'models/' (will be created if it does not exist)\n",
      "    - Model path: models\\BBM1-2layers_OPF-I1-O3_20240201-111317.pt\n",
      "    - Summary path: models/models_summary.csv\n",
      "Training on cuda:0\n",
      "Training starts in:  00:00\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 (Loss - Running: 108364.42, Avg. batch: 8.34e+01, Best avg.: N/A): 100%|██████████| 1304/1304 [00:03<00:00, 355.73it/s]\n",
      "Epoch 2/100 (Loss - Running: 42921.78, Avg. batch: 3.30e+01, Best avg.: 40.91): 100%|██████████| 1304/1304 [00:03<00:00, 363.00it/s]\n",
      "Epoch 3/100 (Loss - Running: 42141.25, Avg. batch: 3.24e+01, Best avg.: 37.92): 100%|██████████| 1304/1304 [00:03<00:00, 363.23it/s]\n",
      "Epoch 4/100 (Loss - Running: 40906.66, Avg. batch: 3.15e+01, Best avg.: 37.42): 100%|██████████| 1304/1304 [00:04<00:00, 318.98it/s]\n",
      "Epoch 5/100 (Loss - Running: 37906.08, Avg. batch: 2.92e+01, Best avg.: 36.41): 100%|██████████| 1304/1304 [00:03<00:00, 327.87it/s]\n",
      "Epoch 6/100 (Loss - Running: 33244.90, Avg. batch: 2.56e+01, Best avg.: 31.29): 100%|██████████| 1304/1304 [00:03<00:00, 328.37it/s]\n",
      "Epoch 7/100 (Loss - Running: 28678.70, Avg. batch: 2.21e+01, Best avg.: 25.16): 100%|██████████| 1304/1304 [00:04<00:00, 318.42it/s]\n",
      "Epoch 8/100 (Loss - Running: 26155.95, Avg. batch: 2.01e+01, Best avg.: 21.15): 100%|██████████| 1304/1304 [00:03<00:00, 347.60it/s]\n",
      "Epoch 9/100 (Loss - Running: 25379.37, Avg. batch: 1.95e+01, Best avg.: 19.09): 100%|██████████| 1304/1304 [00:04<00:00, 304.05it/s]\n",
      "Epoch 10/100 (Loss - Running: 25354.18, Avg. batch: 1.95e+01, Best avg.: 18.66): 100%|██████████| 1304/1304 [00:04<00:00, 310.97it/s]\n",
      "Epoch 11/100 (Loss - Running: 25233.95, Avg. batch: 1.94e+01, Best avg.: 18.32): 100%|██████████| 1304/1304 [00:04<00:00, 315.16it/s]\n",
      "Epoch 12/100 (Loss - Running: 25292.39, Avg. batch: 1.95e+01, Best avg.: 18.32): 100%|██████████| 1304/1304 [00:03<00:00, 343.95it/s]\n",
      "Epoch 13/100 (Loss - Running: 25206.65, Avg. batch: 1.94e+01, Best avg.: 18.32): 100%|██████████| 1304/1304 [00:03<00:00, 356.76it/s]\n",
      "Epoch 14/100 (Loss - Running: 25221.09, Avg. batch: 1.94e+01, Best avg.: 18.27): 100%|██████████| 1304/1304 [00:03<00:00, 359.36it/s]\n",
      "Epoch 15/100 (Loss - Running: 25215.95, Avg. batch: 1.94e+01, Best avg.: 18.27): 100%|██████████| 1304/1304 [00:03<00:00, 346.71it/s]\n",
      "Epoch 16/100 (Loss - Running: 25166.43, Avg. batch: 1.94e+01, Best avg.: 18.27): 100%|██████████| 1304/1304 [00:03<00:00, 366.24it/s]\n",
      "Epoch 17/100 (Loss - Running: 25125.68, Avg. batch: 1.93e+01, Best avg.: 18.27): 100%|██████████| 1304/1304 [00:03<00:00, 368.99it/s]\n",
      "Epoch 18/100 (Loss - Running: 25083.65, Avg. batch: 1.93e+01, Best avg.: 18.27): 100%|██████████| 1304/1304 [00:03<00:00, 358.07it/s]\n",
      "Epoch 19/100 (Loss - Running: 25100.97, Avg. batch: 1.93e+01, Best avg.: 18.27): 100%|██████████| 1304/1304 [00:04<00:00, 313.82it/s]\n",
      "Epoch 20/100 (Loss - Running: 25162.23, Avg. batch: 1.94e+01, Best avg.: 18.10): 100%|██████████| 1304/1304 [00:04<00:00, 317.28it/s]\n",
      "Epoch 21/100 (Loss - Running: 25078.18, Avg. batch: 1.93e+01, Best avg.: 18.10): 100%|██████████| 1304/1304 [00:03<00:00, 351.39it/s]\n",
      "Epoch 22/100 (Loss - Running: 24999.93, Avg. batch: 1.92e+01, Best avg.: 18.10): 100%|██████████| 1304/1304 [00:03<00:00, 334.87it/s]\n",
      "Epoch 23/100 (Loss - Running: 25010.49, Avg. batch: 1.92e+01, Best avg.: 18.10): 100%|██████████| 1304/1304 [00:04<00:00, 317.33it/s]\n",
      "Epoch 24/100 (Loss - Running: 25019.80, Avg. batch: 1.92e+01, Best avg.: 18.10): 100%|██████████| 1304/1304 [00:03<00:00, 338.87it/s]\n",
      "Epoch 25/100 (Loss - Running: 24959.56, Avg. batch: 1.92e+01, Best avg.: 18.10): 100%|██████████| 1304/1304 [00:03<00:00, 360.36it/s]\n",
      "Epoch 26/100 (Loss - Running: 24943.71, Avg. batch: 1.92e+01, Best avg.: 18.10): 100%|██████████| 1304/1304 [00:03<00:00, 334.19it/s]\n",
      "Epoch 27/100 (Loss - Running: 24986.77, Avg. batch: 1.92e+01, Best avg.: 18.10): 100%|██████████| 1304/1304 [00:03<00:00, 327.94it/s]\n",
      "Epoch 28/100 (Loss - Running: 24984.71, Avg. batch: 1.92e+01, Best avg.: 18.09): 100%|██████████| 1304/1304 [00:03<00:00, 337.97it/s]\n",
      "Epoch 29/100 (Loss - Running: 24906.92, Avg. batch: 1.92e+01, Best avg.: 18.09): 100%|██████████| 1304/1304 [00:03<00:00, 335.98it/s]\n",
      "Epoch 30/100 (Loss - Running: 24924.76, Avg. batch: 1.92e+01, Best avg.: 18.09): 100%|██████████| 1304/1304 [00:03<00:00, 342.16it/s]\n",
      "Epoch 31/100 (Loss - Running: 24903.90, Avg. batch: 1.92e+01, Best avg.: 18.09): 100%|██████████| 1304/1304 [00:03<00:00, 343.61it/s]\n",
      "Epoch 32/100 (Loss - Running: 24694.04, Avg. batch: 1.90e+01, Best avg.: 18.07): 100%|██████████| 1304/1304 [00:03<00:00, 365.84it/s]\n",
      "Epoch 33/100 (Loss - Running: 24857.90, Avg. batch: 1.91e+01, Best avg.: 18.07): 100%|██████████| 1304/1304 [00:03<00:00, 372.57it/s]\n",
      "Epoch 34/100 (Loss - Running: 24893.57, Avg. batch: 1.91e+01, Best avg.: 18.07): 100%|██████████| 1304/1304 [00:03<00:00, 365.00it/s]\n",
      "Epoch 35/100 (Loss - Running: 24790.38, Avg. batch: 1.91e+01, Best avg.: 18.07): 100%|██████████| 1304/1304 [00:03<00:00, 378.03it/s]\n",
      "Epoch 36/100 (Loss - Running: 24798.08, Avg. batch: 1.91e+01, Best avg.: 17.99): 100%|██████████| 1304/1304 [00:03<00:00, 333.38it/s]\n",
      "Epoch 37/100 (Loss - Running: 24843.29, Avg. batch: 1.91e+01, Best avg.: 17.99): 100%|██████████| 1304/1304 [00:04<00:00, 324.31it/s]\n",
      "Epoch 38/100 (Loss - Running: 24795.53, Avg. batch: 1.91e+01, Best avg.: 17.99): 100%|██████████| 1304/1304 [00:03<00:00, 372.53it/s]\n",
      "Epoch 39/100 (Loss - Running: 24826.85, Avg. batch: 1.91e+01, Best avg.: 17.99): 100%|██████████| 1304/1304 [00:03<00:00, 335.24it/s]\n",
      "Epoch 40/100 (Loss - Running: 24792.30, Avg. batch: 1.91e+01, Best avg.: 17.99): 100%|██████████| 1304/1304 [00:03<00:00, 335.61it/s]\n",
      "Epoch 41/100 (Loss - Running: 24779.92, Avg. batch: 1.91e+01, Best avg.: 17.99): 100%|██████████| 1304/1304 [00:03<00:00, 367.18it/s]\n",
      "Epoch 42/100 (Loss - Running: 24740.75, Avg. batch: 1.90e+01, Best avg.: 17.97): 100%|██████████| 1304/1304 [00:03<00:00, 370.45it/s]\n",
      "Epoch 43/100 (Loss - Running: 24732.37, Avg. batch: 1.90e+01, Best avg.: 17.97): 100%|██████████| 1304/1304 [00:03<00:00, 372.36it/s]\n",
      "Epoch 44/100 (Loss - Running: 24823.75, Avg. batch: 1.91e+01, Best avg.: 17.97): 100%|██████████| 1304/1304 [00:03<00:00, 373.25it/s]\n",
      "Epoch 45/100 (Loss - Running: 24712.59, Avg. batch: 1.90e+01, Best avg.: 17.97): 100%|██████████| 1304/1304 [00:03<00:00, 341.38it/s]\n",
      "Epoch 46/100 (Loss - Running: 24684.78, Avg. batch: 1.90e+01, Best avg.: 17.97): 100%|██████████| 1304/1304 [00:04<00:00, 307.16it/s]\n",
      "Epoch 47/100 (Loss - Running: 24703.88, Avg. batch: 1.90e+01, Best avg.: 17.97): 100%|██████████| 1304/1304 [00:03<00:00, 360.56it/s]\n",
      "Epoch 48/100 (Loss - Running: 24676.79, Avg. batch: 1.90e+01, Best avg.: 17.97): 100%|██████████| 1304/1304 [00:03<00:00, 349.88it/s]\n",
      "Epoch 49/100 (Loss - Running: 24620.39, Avg. batch: 1.89e+01, Best avg.: 17.97): 100%|██████████| 1304/1304 [00:04<00:00, 295.93it/s]\n",
      "Epoch 50/100 (Loss - Running: 24623.43, Avg. batch: 1.89e+01, Best avg.: 17.97): 100%|██████████| 1304/1304 [00:04<00:00, 291.92it/s]\n",
      "Epoch 51/100 (Loss - Running: 24601.06, Avg. batch: 1.89e+01, Best avg.: 17.97): 100%|██████████| 1304/1304 [00:03<00:00, 358.64it/s]\n",
      "Epoch 52/100 (Loss - Running: 24684.59, Avg. batch: 1.90e+01, Best avg.: 17.96): 100%|██████████| 1304/1304 [00:03<00:00, 369.43it/s]\n",
      "Epoch 53/100 (Loss - Running: 24633.26, Avg. batch: 1.89e+01, Best avg.: 17.96): 100%|██████████| 1304/1304 [00:03<00:00, 336.19it/s]\n",
      "Epoch 54/100 (Loss - Running: 24615.50, Avg. batch: 1.89e+01, Best avg.: 17.95): 100%|██████████| 1304/1304 [00:03<00:00, 347.70it/s]\n",
      "Epoch 55/100 (Loss - Running: 24552.73, Avg. batch: 1.89e+01, Best avg.: 17.95): 100%|██████████| 1304/1304 [00:03<00:00, 345.95it/s]\n",
      "Epoch 56/100 (Loss - Running: 24613.46, Avg. batch: 1.89e+01, Best avg.: 17.95): 100%|██████████| 1304/1304 [00:03<00:00, 342.19it/s]\n",
      "Epoch 57/100 (Loss - Running: 24565.73, Avg. batch: 1.89e+01, Best avg.: 17.95): 100%|██████████| 1304/1304 [00:04<00:00, 325.15it/s]\n",
      "Epoch 58/100 (Loss - Running: 24549.91, Avg. batch: 1.89e+01, Best avg.: 17.90): 100%|██████████| 1304/1304 [00:03<00:00, 340.88it/s]\n",
      "Epoch 59/100 (Loss - Running: 24493.15, Avg. batch: 1.88e+01, Best avg.: 17.90): 100%|██████████| 1304/1304 [00:03<00:00, 344.32it/s]\n",
      "Epoch 60/100 (Loss - Running: 24598.83, Avg. batch: 1.89e+01, Best avg.: 17.90): 100%|██████████| 1304/1304 [00:03<00:00, 369.08it/s]\n",
      "Epoch 61/100 (Loss - Running: 24561.02, Avg. batch: 1.89e+01, Best avg.: 17.90): 100%|██████████| 1304/1304 [00:03<00:00, 354.45it/s]\n",
      "Epoch 62/100 (Loss - Running: 24501.41, Avg. batch: 1.88e+01, Best avg.: 17.90): 100%|██████████| 1304/1304 [00:03<00:00, 338.59it/s]\n",
      "Epoch 63/100 (Loss - Running: 24512.46, Avg. batch: 1.89e+01, Best avg.: 17.90): 100%|██████████| 1304/1304 [00:04<00:00, 320.43it/s]\n",
      "Epoch 64/100 (Loss - Running: 24504.17, Avg. batch: 1.88e+01, Best avg.: 17.90): 100%|██████████| 1304/1304 [00:03<00:00, 329.95it/s]\n",
      "Epoch 65/100 (Loss - Running: 24467.04, Avg. batch: 1.88e+01, Best avg.: 17.90): 100%|██████████| 1304/1304 [00:03<00:00, 329.57it/s]\n",
      "Epoch 66/100 (Loss - Running: 24443.58, Avg. batch: 1.88e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 356.75it/s]\n",
      "Epoch 67/100 (Loss - Running: 24434.90, Avg. batch: 1.88e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 348.12it/s]\n",
      "Epoch 68/100 (Loss - Running: 24456.96, Avg. batch: 1.88e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 361.38it/s]\n",
      "Epoch 69/100 (Loss - Running: 24456.26, Avg. batch: 1.88e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 364.13it/s]\n",
      "Epoch 70/100 (Loss - Running: 24454.85, Avg. batch: 1.88e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 347.20it/s]\n",
      "Epoch 71/100 (Loss - Running: 24263.16, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 365.58it/s]\n",
      "Epoch 72/100 (Loss - Running: 24437.63, Avg. batch: 1.88e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 334.84it/s]\n",
      "Epoch 73/100 (Loss - Running: 24427.62, Avg. batch: 1.88e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 361.58it/s]\n",
      "Epoch 74/100 (Loss - Running: 24474.94, Avg. batch: 1.88e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 347.29it/s]\n",
      "Epoch 75/100 (Loss - Running: 24383.08, Avg. batch: 1.88e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 372.39it/s]\n",
      "Epoch 76/100 (Loss - Running: 24406.54, Avg. batch: 1.88e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 363.03it/s]\n",
      "Epoch 77/100 (Loss - Running: 24411.07, Avg. batch: 1.88e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 344.14it/s]\n",
      "Epoch 78/100 (Loss - Running: 24430.18, Avg. batch: 1.88e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 333.64it/s]\n",
      "Epoch 79/100 (Loss - Running: 24377.68, Avg. batch: 1.88e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 347.61it/s]\n",
      "Epoch 80/100 (Loss - Running: 24379.64, Avg. batch: 1.88e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 356.65it/s]\n",
      "Epoch 81/100 (Loss - Running: 24397.77, Avg. batch: 1.88e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 351.36it/s]\n",
      "Epoch 82/100 (Loss - Running: 24372.71, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:04<00:00, 305.48it/s]\n",
      "Epoch 83/100 (Loss - Running: 24368.58, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 342.45it/s]\n",
      "Epoch 84/100 (Loss - Running: 24324.42, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:04<00:00, 322.89it/s]\n",
      "Epoch 85/100 (Loss - Running: 24335.53, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 351.97it/s]\n",
      "Epoch 86/100 (Loss - Running: 24351.69, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 334.42it/s]\n",
      "Epoch 87/100 (Loss - Running: 24330.44, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 355.50it/s]\n",
      "Epoch 88/100 (Loss - Running: 24268.11, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 353.68it/s]\n",
      "Epoch 89/100 (Loss - Running: 24388.75, Avg. batch: 1.88e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 329.38it/s]\n",
      "Epoch 90/100 (Loss - Running: 24329.93, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:04<00:00, 285.43it/s]\n",
      "Epoch 91/100 (Loss - Running: 24310.97, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:04<00:00, 321.76it/s]\n",
      "Epoch 92/100 (Loss - Running: 24318.69, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 342.13it/s]\n",
      "Epoch 93/100 (Loss - Running: 24299.64, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 326.38it/s]\n",
      "Epoch 94/100 (Loss - Running: 24350.98, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 342.32it/s]\n",
      "Epoch 95/100 (Loss - Running: 24290.61, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:04<00:00, 325.27it/s]\n",
      "Epoch 96/100 (Loss - Running: 24353.82, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 332.55it/s]\n",
      "Epoch 97/100 (Loss - Running: 24312.98, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 341.21it/s]\n",
      "Epoch 98/100 (Loss - Running: 24261.39, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:04<00:00, 308.64it/s]\n",
      "Epoch 99/100 (Loss - Running: 24313.95, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:03<00:00, 329.42it/s]\n",
      "Epoch 100/100 (Loss - Running: 24325.75, Avg. batch: 1.87e+01, Best avg.: 17.80): 100%|██████████| 1304/1304 [00:04<00:00, 286.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Train loss  Validation loss  \\\n",
      "Model        Dataset   Timestamp                                      \n",
      "BBM1-2layers OPF-I1-O3 20240201-111317   19.041072        18.338107   \n",
      "\n",
      "                                        Test loss  Input size  Output size  \\\n",
      "Model        Dataset   Timestamp                                             \n",
      "BBM1-2layers OPF-I1-O3 20240201-111317  18.260585          53            9   \n",
      "\n",
      "                                        Training time [ms]  \\\n",
      "Model        Dataset   Timestamp                             \n",
      "BBM1-2layers OPF-I1-O3 20240201-111317          422910.943   \n",
      "\n",
      "                                                                              Model path  \n",
      "Model        Dataset   Timestamp                                                          \n",
      "BBM1-2layers OPF-I1-O3 20240201-111317  models\\BBM1-2layers_OPF-I1-O3_20240201-111317.pt  \n",
      "------ Finished! ------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\projects\\IPTLC_BBMs\\src\\creator.py:405: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  if comparison:\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "BBM_creator.train(save_to=\"models\", epochs=100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T10:20:27.388460300Z",
     "start_time": "2024-02-01T10:13:17.793676Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        Train loss  Validation loss  \\\nModel        Dataset   Timestamp                                      \nBBM1-2layers OPF-I1-O3 20240201-111317   19.041072        18.338107   \n\n                                        Test loss  Input size  Output size  \\\nModel        Dataset   Timestamp                                             \nBBM1-2layers OPF-I1-O3 20240201-111317  18.260585          53            9   \n\n                                        Training time [ms]  \\\nModel        Dataset   Timestamp                             \nBBM1-2layers OPF-I1-O3 20240201-111317          422910.943   \n\n                                                                              Model path  \nModel        Dataset   Timestamp                                                          \nBBM1-2layers OPF-I1-O3 20240201-111317  models\\BBM1-2layers_OPF-I1-O3_20240201-111317.pt  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>Train loss</th>\n      <th>Validation loss</th>\n      <th>Test loss</th>\n      <th>Input size</th>\n      <th>Output size</th>\n      <th>Training time [ms]</th>\n      <th>Model path</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th>Dataset</th>\n      <th>Timestamp</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>BBM1-2layers</th>\n      <th>OPF-I1-O3</th>\n      <th>20240201-111317</th>\n      <td>19.041072</td>\n      <td>18.338107</td>\n      <td>18.260585</td>\n      <td>53</td>\n      <td>9</td>\n      <td>422910.943</td>\n      <td>models\\BBM1-2layers_OPF-I1-O3_20240201-111317.pt</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BBM_creator._summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T10:20:29.965630800Z",
     "start_time": "2024-02-01T10:20:27.394977700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BBM 2: a two-layer feedforward neural network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalized dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assign the model\n",
    "BBM2 = opf_bbm.BBM2_DeepNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: data\\OPF\\2023-12-06_18-00-46\\OPF-NI1-O3\n",
      "---- Dataset loaded ----\n",
      "Input shape: (59620, 53)\n",
      "Output shape: (59620, 9)\n",
      "---- Removing NaNs ----\n",
      "Rows to delete: [57886 57887 57888 57889 57890 57891 57892 57893 57894 57895 57896 57897\n",
      " 57898 57899 57900 57901 57902 57903 57904 57905 57906 57907 57908 57909\n",
      " 57910 57911 57912 57913 57914 57915 57916 57917 57918 57919 57920 57921\n",
      " 57922 57923 57924 57925 57926 57927 57928 57929 57930]\n",
      "Input shape: (59575, 53)\n",
      "Output shape: (59575, 9)\n",
      "---- Converting to torch tensors ----\n",
      "---- Dataset loaded! ----\n"
     ]
    }
   ],
   "source": [
    "# Set up the dataset\n",
    "dataset_name = \"OPF-NI1-O3\"\n",
    "loaders = creator.setup_datasets(datasets_folder, dataset_name, remove_nans=True, ratios=(0.70, 0.15, 0.15), batch_size=32)\n",
    "BBM_creator.set_dataloaders(*loaders)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:37:42.439487600Z",
     "start_time": "2024-02-01T15:37:42.284525Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "input_size = loaders[0].dataset[0][0].shape[0]\n",
    "output_size = loaders[0].dataset[0][1].shape[0]\n",
    "BBM_creator.instantiate_model(BBM2, input_size, output_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T10:20:30.227452800Z",
     "start_time": "2024-02-01T10:20:30.068841700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Training model 'BBM2-deep' ------\n",
      "Models and summary will be save to 'models/' (will be created if it does not exist)\n",
      "    - Model path: models\\BBM2-deep_OPF-NI1-O3_20240201-112030.pt\n",
      "    - Summary path: models/models_summary.csv\n",
      "Training on cuda:0\n",
      "Training starts in:  00:00\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 (Loss - Running: 177425.19, Avg. batch: 1.36e+02, Best avg.: N/A): 100%|██████████| 1304/1304 [00:05<00:00, 238.40it/s]\n",
      "Epoch 2/500 (Loss - Running: 91348.16, Avg. batch: 7.03e+01, Best avg.: 69.84): 100%|██████████| 1304/1304 [00:05<00:00, 228.02it/s]\n",
      "Epoch 3/500 (Loss - Running: 83909.76, Avg. batch: 6.45e+01, Best avg.: 69.84): 100%|██████████| 1304/1304 [00:05<00:00, 223.66it/s]\n",
      "Epoch 4/500 (Loss - Running: 75796.01, Avg. batch: 5.83e+01, Best avg.: 69.84): 100%|██████████| 1304/1304 [00:05<00:00, 241.03it/s]\n",
      "Epoch 5/500 (Loss - Running: 69199.15, Avg. batch: 5.32e+01, Best avg.: 54.34): 100%|██████████| 1304/1304 [00:05<00:00, 245.58it/s]\n",
      "Epoch 6/500 (Loss - Running: 61228.89, Avg. batch: 4.71e+01, Best avg.: 48.33): 100%|██████████| 1304/1304 [00:05<00:00, 250.36it/s]\n",
      "Epoch 7/500 (Loss - Running: 57830.33, Avg. batch: 4.45e+01, Best avg.: 47.53): 100%|██████████| 1304/1304 [00:05<00:00, 228.74it/s]\n",
      "Epoch 8/500 (Loss - Running: 53982.57, Avg. batch: 4.15e+01, Best avg.: 46.07): 100%|██████████| 1304/1304 [00:06<00:00, 213.46it/s]\n",
      "Epoch 9/500 (Loss - Running: 50024.63, Avg. batch: 3.85e+01, Best avg.: 40.00): 100%|██████████| 1304/1304 [00:05<00:00, 246.99it/s]\n",
      "Epoch 10/500 (Loss - Running: 46890.95, Avg. batch: 3.61e+01, Best avg.: 36.86): 100%|██████████| 1304/1304 [00:05<00:00, 236.51it/s]\n",
      "Epoch 11/500 (Loss - Running: 43554.11, Avg. batch: 3.35e+01, Best avg.: 36.86): 100%|██████████| 1304/1304 [00:05<00:00, 243.73it/s]\n",
      "Epoch 12/500 (Loss - Running: 40176.97, Avg. batch: 3.09e+01, Best avg.: 32.74): 100%|██████████| 1304/1304 [00:05<00:00, 229.18it/s]\n",
      "Epoch 13/500 (Loss - Running: 36914.19, Avg. batch: 2.84e+01, Best avg.: 29.27): 100%|██████████| 1304/1304 [00:05<00:00, 238.79it/s]\n",
      "Epoch 14/500 (Loss - Running: 35003.41, Avg. batch: 2.69e+01, Best avg.: 29.27): 100%|██████████| 1304/1304 [00:06<00:00, 196.48it/s]\n",
      "Epoch 15/500 (Loss - Running: 33851.53, Avg. batch: 2.60e+01, Best avg.: 29.27): 100%|██████████| 1304/1304 [00:06<00:00, 210.65it/s]\n",
      "Epoch 16/500 (Loss - Running: 32738.27, Avg. batch: 2.52e+01, Best avg.: 25.54): 100%|██████████| 1304/1304 [00:05<00:00, 232.94it/s]\n",
      "Epoch 17/500 (Loss - Running: 32163.58, Avg. batch: 2.47e+01, Best avg.: 25.54): 100%|██████████| 1304/1304 [00:05<00:00, 237.84it/s]\n",
      "Epoch 18/500 (Loss - Running: 31981.06, Avg. batch: 2.46e+01, Best avg.: 25.54): 100%|██████████| 1304/1304 [00:05<00:00, 224.60it/s]\n",
      "Epoch 19/500 (Loss - Running: 30839.06, Avg. batch: 2.37e+01, Best avg.: 24.52): 100%|██████████| 1304/1304 [00:05<00:00, 220.24it/s]\n",
      "Epoch 20/500 (Loss - Running: 31254.40, Avg. batch: 2.40e+01, Best avg.: 23.94): 100%|██████████| 1304/1304 [00:05<00:00, 224.40it/s]\n",
      "Epoch 21/500 (Loss - Running: 30117.64, Avg. batch: 2.32e+01, Best avg.: 23.87): 100%|██████████| 1304/1304 [00:05<00:00, 227.46it/s]\n",
      "Epoch 22/500 (Loss - Running: 30069.72, Avg. batch: 2.31e+01, Best avg.: 22.98): 100%|██████████| 1304/1304 [00:05<00:00, 237.72it/s]\n",
      "Epoch 23/500 (Loss - Running: 31043.82, Avg. batch: 2.39e+01, Best avg.: 22.98): 100%|██████████| 1304/1304 [00:05<00:00, 224.75it/s]\n",
      "Epoch 24/500 (Loss - Running: 29870.13, Avg. batch: 2.30e+01, Best avg.: 22.98): 100%|██████████| 1304/1304 [00:05<00:00, 221.39it/s]\n",
      "Epoch 25/500 (Loss - Running: 29674.52, Avg. batch: 2.28e+01, Best avg.: 22.98): 100%|██████████| 1304/1304 [00:06<00:00, 213.01it/s]\n",
      "Epoch 26/500 (Loss - Running: 29672.66, Avg. batch: 2.28e+01, Best avg.: 22.81): 100%|██████████| 1304/1304 [00:05<00:00, 223.96it/s]\n",
      "Epoch 27/500 (Loss - Running: 29744.14, Avg. batch: 2.29e+01, Best avg.: 22.53): 100%|██████████| 1304/1304 [00:06<00:00, 207.04it/s]\n",
      "Epoch 28/500 (Loss - Running: 29957.25, Avg. batch: 2.30e+01, Best avg.: 22.53): 100%|██████████| 1304/1304 [00:05<00:00, 222.90it/s]\n",
      "Epoch 29/500 (Loss - Running: 30033.61, Avg. batch: 2.31e+01, Best avg.: 22.53): 100%|██████████| 1304/1304 [00:05<00:00, 225.76it/s]\n",
      "Epoch 30/500 (Loss - Running: 30125.14, Avg. batch: 2.32e+01, Best avg.: 22.53): 100%|██████████| 1304/1304 [00:05<00:00, 234.90it/s]\n",
      "Epoch 31/500 (Loss - Running: 29367.07, Avg. batch: 2.26e+01, Best avg.: 22.53): 100%|██████████| 1304/1304 [00:05<00:00, 227.24it/s]\n",
      "Epoch 32/500 (Loss - Running: 29345.82, Avg. batch: 2.26e+01, Best avg.: 22.25): 100%|██████████| 1304/1304 [00:05<00:00, 219.60it/s]\n",
      "Epoch 33/500 (Loss - Running: 29505.01, Avg. batch: 2.27e+01, Best avg.: 22.25): 100%|██████████| 1304/1304 [00:05<00:00, 218.81it/s]\n",
      "Epoch 34/500 (Loss - Running: 30578.07, Avg. batch: 2.35e+01, Best avg.: 22.25): 100%|██████████| 1304/1304 [00:06<00:00, 199.51it/s]\n",
      "Epoch 35/500 (Loss - Running: 30143.17, Avg. batch: 2.32e+01, Best avg.: 22.25): 100%|██████████| 1304/1304 [00:05<00:00, 219.16it/s]\n",
      "Epoch 36/500 (Loss - Running: 30581.01, Avg. batch: 2.35e+01, Best avg.: 22.25): 100%|██████████| 1304/1304 [00:05<00:00, 224.49it/s]\n",
      "Epoch 37/500 (Loss - Running: 29871.81, Avg. batch: 2.30e+01, Best avg.: 22.25): 100%|██████████| 1304/1304 [00:05<00:00, 233.94it/s]\n",
      "Epoch 38/500 (Loss - Running: 30396.81, Avg. batch: 2.34e+01, Best avg.: 22.25): 100%|██████████| 1304/1304 [00:05<00:00, 235.83it/s]\n",
      "Epoch 39/500 (Loss - Running: 29483.88, Avg. batch: 2.27e+01, Best avg.: 22.25): 100%|██████████| 1304/1304 [00:05<00:00, 224.38it/s]\n",
      "Epoch 40/500 (Loss - Running: 29135.64, Avg. batch: 2.24e+01, Best avg.: 22.25): 100%|██████████| 1304/1304 [00:05<00:00, 218.80it/s]\n",
      "Epoch 41/500 (Loss - Running: 29208.56, Avg. batch: 2.25e+01, Best avg.: 22.25): 100%|██████████| 1304/1304 [00:05<00:00, 235.93it/s]\n",
      "Epoch 42/500 (Loss - Running: 28939.87, Avg. batch: 2.23e+01, Best avg.: 22.25): 100%|██████████| 1304/1304 [00:05<00:00, 229.86it/s]\n",
      "Epoch 43/500 (Loss - Running: 28918.02, Avg. batch: 2.22e+01, Best avg.: 22.00): 100%|██████████| 1304/1304 [00:05<00:00, 236.30it/s]\n",
      "Epoch 44/500 (Loss - Running: 29421.41, Avg. batch: 2.26e+01, Best avg.: 22.00): 100%|██████████| 1304/1304 [00:05<00:00, 234.50it/s]\n",
      "Epoch 45/500 (Loss - Running: 27506.67, Avg. batch: 2.12e+01, Best avg.: 21.85): 100%|██████████| 1304/1304 [00:05<00:00, 259.51it/s]\n",
      "Epoch 46/500 (Loss - Running: 28076.65, Avg. batch: 2.16e+01, Best avg.: 21.85): 100%|██████████| 1304/1304 [00:04<00:00, 261.34it/s]\n",
      "Epoch 47/500 (Loss - Running: 29326.80, Avg. batch: 2.26e+01, Best avg.: 21.85): 100%|██████████| 1304/1304 [00:05<00:00, 235.32it/s]\n",
      "Epoch 48/500 (Loss - Running: 29618.61, Avg. batch: 2.28e+01, Best avg.: 21.85): 100%|██████████| 1304/1304 [00:05<00:00, 226.33it/s]\n",
      "Epoch 49/500 (Loss - Running: 29386.49, Avg. batch: 2.26e+01, Best avg.: 21.85): 100%|██████████| 1304/1304 [00:05<00:00, 235.45it/s]\n",
      "Epoch 50/500 (Loss - Running: 28426.31, Avg. batch: 2.19e+01, Best avg.: 21.85): 100%|██████████| 1304/1304 [00:05<00:00, 218.49it/s]\n",
      "Epoch 51/500 (Loss - Running: 28673.52, Avg. batch: 2.21e+01, Best avg.: 21.85): 100%|██████████| 1304/1304 [00:05<00:00, 222.63it/s]\n",
      "Epoch 52/500 (Loss - Running: 27687.69, Avg. batch: 2.13e+01, Best avg.: 21.85): 100%|██████████| 1304/1304 [00:05<00:00, 219.55it/s]\n",
      "Epoch 53/500 (Loss - Running: 28947.34, Avg. batch: 2.23e+01, Best avg.: 21.81): 100%|██████████| 1304/1304 [00:05<00:00, 219.49it/s]\n",
      "Epoch 54/500 (Loss - Running: 28382.24, Avg. batch: 2.18e+01, Best avg.: 21.81): 100%|██████████| 1304/1304 [00:05<00:00, 230.33it/s]\n",
      "Epoch 55/500 (Loss - Running: 28583.39, Avg. batch: 2.20e+01, Best avg.: 21.81): 100%|██████████| 1304/1304 [00:05<00:00, 228.21it/s]\n",
      "Epoch 56/500 (Loss - Running: 28292.90, Avg. batch: 2.18e+01, Best avg.: 21.81): 100%|██████████| 1304/1304 [00:05<00:00, 234.84it/s]\n",
      "Epoch 57/500 (Loss - Running: 27914.51, Avg. batch: 2.15e+01, Best avg.: 21.81): 100%|██████████| 1304/1304 [00:05<00:00, 225.16it/s]\n",
      "Epoch 58/500 (Loss - Running: 28701.56, Avg. batch: 2.21e+01, Best avg.: 21.81): 100%|██████████| 1304/1304 [00:05<00:00, 222.65it/s]\n",
      "Epoch 59/500 (Loss - Running: 28871.08, Avg. batch: 2.22e+01, Best avg.: 21.81): 100%|██████████| 1304/1304 [00:05<00:00, 228.55it/s]\n",
      "Epoch 60/500 (Loss - Running: 28951.09, Avg. batch: 2.23e+01, Best avg.: 21.81): 100%|██████████| 1304/1304 [00:05<00:00, 249.62it/s]\n",
      "Epoch 61/500 (Loss - Running: 27950.55, Avg. batch: 2.15e+01, Best avg.: 21.81): 100%|██████████| 1304/1304 [00:05<00:00, 257.39it/s]\n",
      "Epoch 62/500 (Loss - Running: 27723.07, Avg. batch: 2.13e+01, Best avg.: 21.81): 100%|██████████| 1304/1304 [00:05<00:00, 239.20it/s]\n",
      "Epoch 63/500 (Loss - Running: 28176.35, Avg. batch: 2.17e+01, Best avg.: 21.81): 100%|██████████| 1304/1304 [00:05<00:00, 251.94it/s]\n",
      "Epoch 64/500 (Loss - Running: 29346.63, Avg. batch: 2.26e+01, Best avg.: 21.81): 100%|██████████| 1304/1304 [00:05<00:00, 242.82it/s]\n",
      "Epoch 65/500 (Loss - Running: 28593.40, Avg. batch: 2.20e+01, Best avg.: 21.81): 100%|██████████| 1304/1304 [00:05<00:00, 253.27it/s]\n",
      "Epoch 66/500 (Loss - Running: 28222.26, Avg. batch: 2.17e+01, Best avg.: 21.81): 100%|██████████| 1304/1304 [00:05<00:00, 260.48it/s]\n",
      "Epoch 67/500 (Loss - Running: 29800.91, Avg. batch: 2.29e+01, Best avg.: 21.81): 100%|██████████| 1304/1304 [00:05<00:00, 242.84it/s]\n",
      "Epoch 68/500 (Loss - Running: 27984.24, Avg. batch: 2.15e+01, Best avg.: 21.81): 100%|██████████| 1304/1304 [00:05<00:00, 220.72it/s]\n",
      "Epoch 69/500 (Loss - Running: 27968.50, Avg. batch: 2.15e+01, Best avg.: 21.75): 100%|██████████| 1304/1304 [00:05<00:00, 232.21it/s]\n",
      "Epoch 70/500 (Loss - Running: 27877.56, Avg. batch: 2.14e+01, Best avg.: 21.75): 100%|██████████| 1304/1304 [00:05<00:00, 233.10it/s]\n",
      "Epoch 71/500 (Loss - Running: 28840.86, Avg. batch: 2.22e+01, Best avg.: 20.94): 100%|██████████| 1304/1304 [00:05<00:00, 225.63it/s]\n",
      "Epoch 72/500 (Loss - Running: 28951.32, Avg. batch: 2.23e+01, Best avg.: 20.94): 100%|██████████| 1304/1304 [00:06<00:00, 205.53it/s]\n",
      "Epoch 73/500 (Loss - Running: 27750.80, Avg. batch: 2.13e+01, Best avg.: 20.94): 100%|██████████| 1304/1304 [00:06<00:00, 213.19it/s]\n",
      "Epoch 74/500 (Loss - Running: 28245.56, Avg. batch: 2.17e+01, Best avg.: 20.94): 100%|██████████| 1304/1304 [00:05<00:00, 222.27it/s]\n",
      "Epoch 75/500 (Loss - Running: 27152.39, Avg. batch: 2.09e+01, Best avg.: 20.94): 100%|██████████| 1304/1304 [00:05<00:00, 224.64it/s]\n",
      "Epoch 76/500 (Loss - Running: 28185.62, Avg. batch: 2.17e+01, Best avg.: 20.94): 100%|██████████| 1304/1304 [00:05<00:00, 233.45it/s]\n",
      "Epoch 77/500 (Loss - Running: 27077.26, Avg. batch: 2.08e+01, Best avg.: 20.94): 100%|██████████| 1304/1304 [00:05<00:00, 232.64it/s]\n",
      "Epoch 78/500 (Loss - Running: 28475.98, Avg. batch: 2.19e+01, Best avg.: 20.94): 100%|██████████| 1304/1304 [00:05<00:00, 228.02it/s]\n",
      "Epoch 79/500 (Loss - Running: 28330.33, Avg. batch: 2.18e+01, Best avg.: 20.94): 100%|██████████| 1304/1304 [00:05<00:00, 227.62it/s]\n",
      "Epoch 80/500 (Loss - Running: 27901.47, Avg. batch: 2.15e+01, Best avg.: 20.94): 100%|██████████| 1304/1304 [00:05<00:00, 227.88it/s]\n",
      "Epoch 81/500 (Loss - Running: 27131.45, Avg. batch: 2.09e+01, Best avg.: 20.94): 100%|██████████| 1304/1304 [00:06<00:00, 208.92it/s]\n",
      "Epoch 82/500 (Loss - Running: 26702.77, Avg. batch: 2.05e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:06<00:00, 211.30it/s]\n",
      "Epoch 83/500 (Loss - Running: 27140.55, Avg. batch: 2.09e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 230.89it/s]\n",
      "Epoch 84/500 (Loss - Running: 27477.21, Avg. batch: 2.11e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 233.72it/s]\n",
      "Epoch 85/500 (Loss - Running: 27006.13, Avg. batch: 2.08e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 230.98it/s]\n",
      "Epoch 86/500 (Loss - Running: 27489.59, Avg. batch: 2.11e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 233.67it/s]\n",
      "Epoch 87/500 (Loss - Running: 27200.20, Avg. batch: 2.09e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 230.28it/s]\n",
      "Epoch 88/500 (Loss - Running: 27144.29, Avg. batch: 2.09e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 232.86it/s]\n",
      "Epoch 89/500 (Loss - Running: 27314.95, Avg. batch: 2.10e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 232.27it/s]\n",
      "Epoch 90/500 (Loss - Running: 27401.24, Avg. batch: 2.11e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 230.56it/s]\n",
      "Epoch 91/500 (Loss - Running: 26862.58, Avg. batch: 2.07e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:06<00:00, 211.69it/s]\n",
      "Epoch 92/500 (Loss - Running: 27495.83, Avg. batch: 2.12e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 224.84it/s]\n",
      "Epoch 93/500 (Loss - Running: 28351.12, Avg. batch: 2.18e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 232.22it/s]\n",
      "Epoch 94/500 (Loss - Running: 27368.17, Avg. batch: 2.11e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 228.14it/s]\n",
      "Epoch 95/500 (Loss - Running: 28343.35, Avg. batch: 2.18e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 232.87it/s]\n",
      "Epoch 96/500 (Loss - Running: 27320.40, Avg. batch: 2.10e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 230.13it/s]\n",
      "Epoch 97/500 (Loss - Running: 28207.25, Avg. batch: 2.17e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 231.14it/s]\n",
      "Epoch 98/500 (Loss - Running: 27355.82, Avg. batch: 2.10e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 218.65it/s]\n",
      "Epoch 99/500 (Loss - Running: 27194.23, Avg. batch: 2.09e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:06<00:00, 216.12it/s]\n",
      "Epoch 100/500 (Loss - Running: 28005.64, Avg. batch: 2.15e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:06<00:00, 215.17it/s]\n",
      "Epoch 101/500 (Loss - Running: 29746.73, Avg. batch: 2.29e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:06<00:00, 216.89it/s]\n",
      "Epoch 102/500 (Loss - Running: 28076.44, Avg. batch: 2.16e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 232.35it/s]\n",
      "Epoch 103/500 (Loss - Running: 27723.65, Avg. batch: 2.13e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 232.43it/s]\n",
      "Epoch 104/500 (Loss - Running: 27227.37, Avg. batch: 2.09e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 238.39it/s]\n",
      "Epoch 105/500 (Loss - Running: 27362.66, Avg. batch: 2.10e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 222.38it/s]\n",
      "Epoch 106/500 (Loss - Running: 27406.53, Avg. batch: 2.11e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 234.11it/s]\n",
      "Epoch 107/500 (Loss - Running: 27345.15, Avg. batch: 2.10e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 231.26it/s]\n",
      "Epoch 108/500 (Loss - Running: 28193.43, Avg. batch: 2.17e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 233.53it/s]\n",
      "Epoch 109/500 (Loss - Running: 27437.78, Avg. batch: 2.11e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 237.01it/s]\n",
      "Epoch 110/500 (Loss - Running: 28442.72, Avg. batch: 2.19e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 229.98it/s]\n",
      "Epoch 111/500 (Loss - Running: 27761.10, Avg. batch: 2.14e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 234.60it/s]\n",
      "Epoch 112/500 (Loss - Running: 27913.67, Avg. batch: 2.15e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 226.84it/s]\n",
      "Epoch 113/500 (Loss - Running: 27310.89, Avg. batch: 2.10e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 233.70it/s]\n",
      "Epoch 114/500 (Loss - Running: 28100.63, Avg. batch: 2.16e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 234.49it/s]\n",
      "Epoch 115/500 (Loss - Running: 27467.63, Avg. batch: 2.11e+01, Best avg.: 20.77): 100%|██████████| 1304/1304 [00:05<00:00, 229.63it/s]\n",
      "Epoch 116/500 (Loss - Running: 27595.24, Avg. batch: 2.12e+01, Best avg.: 20.74): 100%|██████████| 1304/1304 [00:05<00:00, 231.61it/s]\n",
      "Epoch 117/500 (Loss - Running: 27285.37, Avg. batch: 2.10e+01, Best avg.: 20.74): 100%|██████████| 1304/1304 [00:05<00:00, 235.43it/s]\n",
      "Epoch 118/500 (Loss - Running: 27191.99, Avg. batch: 2.09e+01, Best avg.: 20.74): 100%|██████████| 1304/1304 [00:05<00:00, 234.25it/s]\n",
      "Epoch 119/500 (Loss - Running: 27461.12, Avg. batch: 2.11e+01, Best avg.: 20.74): 100%|██████████| 1304/1304 [00:05<00:00, 240.63it/s]\n",
      "Epoch 120/500 (Loss - Running: 28433.28, Avg. batch: 2.19e+01, Best avg.: 20.74): 100%|██████████| 1304/1304 [00:05<00:00, 219.59it/s]\n",
      "Epoch 121/500 (Loss - Running: 27525.23, Avg. batch: 2.12e+01, Best avg.: 20.74): 100%|██████████| 1304/1304 [00:05<00:00, 245.92it/s]\n",
      "Epoch 122/500 (Loss - Running: 26472.24, Avg. batch: 2.04e+01, Best avg.: 20.74): 100%|██████████| 1304/1304 [00:05<00:00, 247.45it/s]\n",
      "Epoch 123/500 (Loss - Running: 27323.23, Avg. batch: 2.10e+01, Best avg.: 20.74): 100%|██████████| 1304/1304 [00:06<00:00, 216.66it/s]\n",
      "Epoch 124/500 (Loss - Running: 27359.62, Avg. batch: 2.10e+01, Best avg.: 20.74): 100%|██████████| 1304/1304 [00:05<00:00, 228.77it/s]\n",
      "Epoch 125/500 (Loss - Running: 27849.78, Avg. batch: 2.14e+01, Best avg.: 20.74): 100%|██████████| 1304/1304 [00:05<00:00, 232.99it/s]\n",
      "Epoch 126/500 (Loss - Running: 27533.40, Avg. batch: 2.12e+01, Best avg.: 20.74): 100%|██████████| 1304/1304 [00:05<00:00, 225.17it/s]\n",
      "Epoch 127/500 (Loss - Running: 28794.99, Avg. batch: 2.21e+01, Best avg.: 20.74): 100%|██████████| 1304/1304 [00:05<00:00, 234.65it/s]\n",
      "Epoch 128/500 (Loss - Running: 27210.30, Avg. batch: 2.09e+01, Best avg.: 20.74): 100%|██████████| 1304/1304 [00:05<00:00, 230.52it/s]\n",
      "Epoch 129/500 (Loss - Running: 27338.08, Avg. batch: 2.10e+01, Best avg.: 20.74): 100%|██████████| 1304/1304 [00:05<00:00, 229.68it/s]\n",
      "Epoch 130/500 (Loss - Running: 2508.69, Avg. batch: 2.51e+01, Best avg.: 20.74):  14%|█▍        | 185/1304 [00:00<00:04, 268.65it/s]"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "BBM_creator.train(save_to=\"models\", epochs=500)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T11:09:53.788260500Z",
     "start_time": "2024-02-01T10:20:30.107466900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "BBM_creator._summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Raw dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "# Set up the dataset\n",
    "dataset_name = \"OPF-I1-O3\"\n",
    "loaders = creator.setup_datasets(datasets_folder, dataset_name, remove_nans=True, ratios=(0.70, 0.15, 0.15), batch_size=32)\n",
    "BBM_creator.set_dataloaders(*loaders)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "input_size = loaders[0].dataset[0][0].shape[0]\n",
    "output_size = loaders[0].dataset[0][1].shape[0]\n",
    "BBM_creator.instantiate_model(BBM2, input_size, output_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 'BBM2_deep'\n",
      "The folder 'models/' will be created if it does not exist\n",
      "Training on cuda:0\n",
      "Training starts in:  00:00\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 (Loss - Running: 143714.25, Avg. batch: 1.11e+02, Best avg.: N/A): 100%|██████████| 1304/1304 [00:05<00:00, 238.15it/s]\n",
      "Epoch 2/1000 (Loss - Running: 105456.75, Avg. batch: 8.11e+01, Best avg.: 88.77): 100%|██████████| 1304/1304 [00:05<00:00, 236.75it/s]\n",
      "Epoch 3/1000 (Loss - Running: 102244.17, Avg. batch: 7.86e+01, Best avg.: 85.87): 100%|██████████| 1304/1304 [00:05<00:00, 238.58it/s]\n",
      "Epoch 4/1000 (Loss - Running: 97576.24, Avg. batch: 7.51e+01, Best avg.: 80.03): 100%|██████████| 1304/1304 [00:05<00:00, 235.61it/s]\n",
      "Epoch 5/1000 (Loss - Running: 95546.73, Avg. batch: 7.35e+01, Best avg.: 76.99): 100%|██████████| 1304/1304 [00:05<00:00, 235.18it/s]\n",
      "Epoch 6/1000 (Loss - Running: 85302.18, Avg. batch: 6.56e+01, Best avg.: 73.84): 100%|██████████| 1304/1304 [00:05<00:00, 233.66it/s]\n",
      "Epoch 7/1000 (Loss - Running: 72473.28, Avg. batch: 5.57e+01, Best avg.: 59.65): 100%|██████████| 1304/1304 [00:05<00:00, 217.44it/s]\n",
      "Epoch 8/1000 (Loss - Running: 67377.93, Avg. batch: 5.18e+01, Best avg.: 57.07): 100%|██████████| 1304/1304 [00:05<00:00, 229.29it/s]\n",
      "Epoch 9/1000 (Loss - Running: 65019.29, Avg. batch: 5.00e+01, Best avg.: 52.51): 100%|██████████| 1304/1304 [00:05<00:00, 233.58it/s]\n",
      "Epoch 10/1000 (Loss - Running: 63538.15, Avg. batch: 4.89e+01, Best avg.: 52.51): 100%|██████████| 1304/1304 [00:05<00:00, 232.17it/s]\n",
      "Epoch 11/1000 (Loss - Running: 61151.74, Avg. batch: 4.70e+01, Best avg.: 49.90): 100%|██████████| 1304/1304 [00:05<00:00, 232.86it/s]\n",
      "Epoch 12/1000 (Loss - Running: 58915.92, Avg. batch: 4.53e+01, Best avg.: 49.90): 100%|██████████| 1304/1304 [00:05<00:00, 232.37it/s]\n",
      "Epoch 13/1000 (Loss - Running: 56485.58, Avg. batch: 4.35e+01, Best avg.: 44.39): 100%|██████████| 1304/1304 [00:05<00:00, 233.97it/s]\n",
      "Epoch 14/1000 (Loss - Running: 54796.01, Avg. batch: 4.22e+01, Best avg.: 41.34): 100%|██████████| 1304/1304 [00:05<00:00, 235.68it/s]\n",
      "Epoch 15/1000 (Loss - Running: 52892.44, Avg. batch: 4.07e+01, Best avg.: 40.70): 100%|██████████| 1304/1304 [00:05<00:00, 234.66it/s]\n",
      "Epoch 16/1000 (Loss - Running: 51488.32, Avg. batch: 3.96e+01, Best avg.: 40.51): 100%|██████████| 1304/1304 [00:05<00:00, 234.51it/s]\n",
      "Epoch 17/1000 (Loss - Running: 50142.52, Avg. batch: 3.86e+01, Best avg.: 40.51): 100%|██████████| 1304/1304 [00:05<00:00, 231.72it/s]\n",
      "Epoch 18/1000 (Loss - Running: 48524.19, Avg. batch: 3.73e+01, Best avg.: 40.51): 100%|██████████| 1304/1304 [00:05<00:00, 235.88it/s]\n",
      "Epoch 19/1000 (Loss - Running: 47171.71, Avg. batch: 3.63e+01, Best avg.: 36.69): 100%|██████████| 1304/1304 [00:05<00:00, 234.89it/s]\n",
      "Epoch 20/1000 (Loss - Running: 44611.37, Avg. batch: 3.43e+01, Best avg.: 34.82): 100%|██████████| 1304/1304 [00:05<00:00, 225.13it/s]\n",
      "Epoch 21/1000 (Loss - Running: 43296.59, Avg. batch: 3.33e+01, Best avg.: 34.82): 100%|██████████| 1304/1304 [00:05<00:00, 232.97it/s]\n",
      "Epoch 22/1000 (Loss - Running: 41763.66, Avg. batch: 3.21e+01, Best avg.: 33.63): 100%|██████████| 1304/1304 [00:05<00:00, 234.48it/s]\n",
      "Epoch 23/1000 (Loss - Running: 40420.80, Avg. batch: 3.11e+01, Best avg.: 33.18): 100%|██████████| 1304/1304 [00:05<00:00, 234.43it/s]\n",
      "Epoch 24/1000 (Loss - Running: 37931.06, Avg. batch: 2.92e+01, Best avg.: 32.32): 100%|██████████| 1304/1304 [00:05<00:00, 234.49it/s]\n",
      "Epoch 25/1000 (Loss - Running: 36398.09, Avg. batch: 2.80e+01, Best avg.: 29.16): 100%|██████████| 1304/1304 [00:05<00:00, 234.02it/s]\n",
      "Epoch 26/1000 (Loss - Running: 34941.28, Avg. batch: 2.69e+01, Best avg.: 26.86): 100%|██████████| 1304/1304 [00:05<00:00, 234.82it/s]\n",
      "Epoch 27/1000 (Loss - Running: 33805.62, Avg. batch: 2.60e+01, Best avg.: 26.86): 100%|██████████| 1304/1304 [00:05<00:00, 232.27it/s]\n",
      "Epoch 28/1000 (Loss - Running: 33365.18, Avg. batch: 2.57e+01, Best avg.: 25.65): 100%|██████████| 1304/1304 [00:05<00:00, 234.34it/s]\n",
      "Epoch 29/1000 (Loss - Running: 38227.06, Avg. batch: 2.94e+01, Best avg.: 25.65): 100%|██████████| 1304/1304 [00:05<00:00, 233.64it/s]\n",
      "Epoch 30/1000 (Loss - Running: 38628.19, Avg. batch: 2.97e+01, Best avg.: 25.65): 100%|██████████| 1304/1304 [00:05<00:00, 230.96it/s]\n",
      "Epoch 31/1000 (Loss - Running: 38032.54, Avg. batch: 2.93e+01, Best avg.: 25.65): 100%|██████████| 1304/1304 [00:05<00:00, 234.42it/s]\n",
      "Epoch 32/1000 (Loss - Running: 38668.60, Avg. batch: 2.97e+01, Best avg.: 25.65): 100%|██████████| 1304/1304 [00:05<00:00, 233.84it/s]\n",
      "Epoch 33/1000 (Loss - Running: 37683.72, Avg. batch: 2.90e+01, Best avg.: 25.65): 100%|██████████| 1304/1304 [00:05<00:00, 235.78it/s]\n",
      "Epoch 34/1000 (Loss - Running: 36788.38, Avg. batch: 2.83e+01, Best avg.: 25.65): 100%|██████████| 1304/1304 [00:05<00:00, 235.31it/s]\n",
      "Epoch 35/1000 (Loss - Running: 34111.79, Avg. batch: 2.62e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.99it/s]\n",
      "Epoch 36/1000 (Loss - Running: 37831.46, Avg. batch: 2.91e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.78it/s]\n",
      "Epoch 37/1000 (Loss - Running: 33612.21, Avg. batch: 2.59e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 230.79it/s]\n",
      "Epoch 38/1000 (Loss - Running: 44551.18, Avg. batch: 3.43e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.12it/s]\n",
      "Epoch 39/1000 (Loss - Running: 39098.74, Avg. batch: 3.01e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 230.63it/s]\n",
      "Epoch 40/1000 (Loss - Running: 41385.55, Avg. batch: 3.18e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 228.80it/s]\n",
      "Epoch 41/1000 (Loss - Running: 44509.68, Avg. batch: 3.42e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.52it/s]\n",
      "Epoch 42/1000 (Loss - Running: 43759.23, Avg. batch: 3.37e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 231.23it/s]\n",
      "Epoch 43/1000 (Loss - Running: 41252.27, Avg. batch: 3.17e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.41it/s]\n",
      "Epoch 44/1000 (Loss - Running: 43264.79, Avg. batch: 3.33e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.71it/s]\n",
      "Epoch 45/1000 (Loss - Running: 40533.04, Avg. batch: 3.12e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.49it/s]\n",
      "Epoch 46/1000 (Loss - Running: 44790.62, Avg. batch: 3.45e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.92it/s]\n",
      "Epoch 47/1000 (Loss - Running: 43631.34, Avg. batch: 3.36e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 230.54it/s]\n",
      "Epoch 48/1000 (Loss - Running: 44760.63, Avg. batch: 3.44e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.84it/s]\n",
      "Epoch 49/1000 (Loss - Running: 41749.35, Avg. batch: 3.21e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 229.08it/s]\n",
      "Epoch 50/1000 (Loss - Running: 40960.49, Avg. batch: 3.15e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 229.14it/s]\n",
      "Epoch 51/1000 (Loss - Running: 48697.77, Avg. batch: 3.75e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.98it/s]\n",
      "Epoch 52/1000 (Loss - Running: 49515.46, Avg. batch: 3.81e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 231.73it/s]\n",
      "Epoch 53/1000 (Loss - Running: 49069.77, Avg. batch: 3.77e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.98it/s]\n",
      "Epoch 54/1000 (Loss - Running: 46705.62, Avg. batch: 3.59e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.72it/s]\n",
      "Epoch 55/1000 (Loss - Running: 45489.62, Avg. batch: 3.50e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.47it/s]\n",
      "Epoch 56/1000 (Loss - Running: 46569.33, Avg. batch: 3.58e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.66it/s]\n",
      "Epoch 57/1000 (Loss - Running: 43305.91, Avg. batch: 3.33e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 228.51it/s]\n",
      "Epoch 58/1000 (Loss - Running: 44918.71, Avg. batch: 3.46e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.00it/s]\n",
      "Epoch 59/1000 (Loss - Running: 43060.58, Avg. batch: 3.31e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 230.74it/s]\n",
      "Epoch 60/1000 (Loss - Running: 42410.27, Avg. batch: 3.26e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 229.48it/s]\n",
      "Epoch 61/1000 (Loss - Running: 43370.48, Avg. batch: 3.34e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.69it/s]\n",
      "Epoch 62/1000 (Loss - Running: 43463.16, Avg. batch: 3.34e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.49it/s]\n",
      "Epoch 63/1000 (Loss - Running: 42253.03, Avg. batch: 3.25e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.54it/s]\n",
      "Epoch 64/1000 (Loss - Running: 41887.77, Avg. batch: 3.22e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.71it/s]\n",
      "Epoch 65/1000 (Loss - Running: 42788.26, Avg. batch: 3.29e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.67it/s]\n",
      "Epoch 66/1000 (Loss - Running: 41528.07, Avg. batch: 3.19e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.17it/s]\n",
      "Epoch 67/1000 (Loss - Running: 41534.93, Avg. batch: 3.19e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 222.81it/s]\n",
      "Epoch 68/1000 (Loss - Running: 40861.53, Avg. batch: 3.14e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 230.83it/s]\n",
      "Epoch 69/1000 (Loss - Running: 39938.49, Avg. batch: 3.07e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 229.82it/s]\n",
      "Epoch 70/1000 (Loss - Running: 42217.07, Avg. batch: 3.25e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 231.04it/s]\n",
      "Epoch 71/1000 (Loss - Running: 40741.48, Avg. batch: 3.13e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.04it/s]\n",
      "Epoch 72/1000 (Loss - Running: 40625.76, Avg. batch: 3.13e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 231.61it/s]\n",
      "Epoch 73/1000 (Loss - Running: 40797.31, Avg. batch: 3.14e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.98it/s]\n",
      "Epoch 74/1000 (Loss - Running: 41512.20, Avg. batch: 3.19e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.86it/s]\n",
      "Epoch 75/1000 (Loss - Running: 40712.17, Avg. batch: 3.13e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.16it/s]\n",
      "Epoch 76/1000 (Loss - Running: 38792.70, Avg. batch: 2.98e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.98it/s]\n",
      "Epoch 77/1000 (Loss - Running: 41773.93, Avg. batch: 3.21e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.11it/s]\n",
      "Epoch 78/1000 (Loss - Running: 39984.51, Avg. batch: 3.08e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.14it/s]\n",
      "Epoch 79/1000 (Loss - Running: 41462.30, Avg. batch: 3.19e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 227.87it/s]\n",
      "Epoch 80/1000 (Loss - Running: 40761.90, Avg. batch: 3.14e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 231.65it/s]\n",
      "Epoch 81/1000 (Loss - Running: 39780.42, Avg. batch: 3.06e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 235.02it/s]\n",
      "Epoch 82/1000 (Loss - Running: 40279.57, Avg. batch: 3.10e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.86it/s]\n",
      "Epoch 83/1000 (Loss - Running: 41167.25, Avg. batch: 3.17e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 229.79it/s]\n",
      "Epoch 84/1000 (Loss - Running: 40538.50, Avg. batch: 3.12e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.19it/s]\n",
      "Epoch 85/1000 (Loss - Running: 39620.09, Avg. batch: 3.05e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.25it/s]\n",
      "Epoch 86/1000 (Loss - Running: 39179.60, Avg. batch: 3.01e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.73it/s]\n",
      "Epoch 87/1000 (Loss - Running: 39457.96, Avg. batch: 3.04e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.29it/s]\n",
      "Epoch 88/1000 (Loss - Running: 38467.46, Avg. batch: 2.96e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.82it/s]\n",
      "Epoch 89/1000 (Loss - Running: 40126.21, Avg. batch: 3.09e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 226.66it/s]\n",
      "Epoch 90/1000 (Loss - Running: 39547.57, Avg. batch: 3.04e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.54it/s]\n",
      "Epoch 91/1000 (Loss - Running: 39514.84, Avg. batch: 3.04e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.48it/s]\n",
      "Epoch 92/1000 (Loss - Running: 39587.68, Avg. batch: 3.05e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.06it/s]\n",
      "Epoch 93/1000 (Loss - Running: 40220.97, Avg. batch: 3.09e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.42it/s]\n",
      "Epoch 94/1000 (Loss - Running: 39739.91, Avg. batch: 3.06e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.85it/s]\n",
      "Epoch 95/1000 (Loss - Running: 39521.94, Avg. batch: 3.04e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.82it/s]\n",
      "Epoch 96/1000 (Loss - Running: 38674.07, Avg. batch: 2.97e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 231.90it/s]\n",
      "Epoch 97/1000 (Loss - Running: 40486.97, Avg. batch: 3.11e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.00it/s]\n",
      "Epoch 98/1000 (Loss - Running: 38828.74, Avg. batch: 2.99e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 230.76it/s]\n",
      "Epoch 99/1000 (Loss - Running: 38379.80, Avg. batch: 2.95e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 228.99it/s]\n",
      "Epoch 100/1000 (Loss - Running: 37235.56, Avg. batch: 2.86e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.91it/s]\n",
      "Epoch 101/1000 (Loss - Running: 39661.35, Avg. batch: 3.05e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.73it/s]\n",
      "Epoch 102/1000 (Loss - Running: 44486.90, Avg. batch: 3.42e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.87it/s]\n",
      "Epoch 103/1000 (Loss - Running: 47478.84, Avg. batch: 3.65e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.80it/s]\n",
      "Epoch 104/1000 (Loss - Running: 47405.28, Avg. batch: 3.65e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.96it/s]\n",
      "Epoch 105/1000 (Loss - Running: 47053.79, Avg. batch: 3.62e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.90it/s]\n",
      "Epoch 106/1000 (Loss - Running: 47332.58, Avg. batch: 3.64e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 231.74it/s]\n",
      "Epoch 107/1000 (Loss - Running: 47143.88, Avg. batch: 3.63e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 235.27it/s]\n",
      "Epoch 108/1000 (Loss - Running: 47100.45, Avg. batch: 3.62e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.51it/s]\n",
      "Epoch 109/1000 (Loss - Running: 46782.87, Avg. batch: 3.60e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 228.46it/s]\n",
      "Epoch 110/1000 (Loss - Running: 46895.27, Avg. batch: 3.61e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.67it/s]\n",
      "Epoch 111/1000 (Loss - Running: 46458.13, Avg. batch: 3.57e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.51it/s]\n",
      "Epoch 112/1000 (Loss - Running: 45886.06, Avg. batch: 3.53e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.52it/s]\n",
      "Epoch 113/1000 (Loss - Running: 45794.12, Avg. batch: 3.52e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 235.05it/s]\n",
      "Epoch 114/1000 (Loss - Running: 46516.09, Avg. batch: 3.58e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.77it/s]\n",
      "Epoch 115/1000 (Loss - Running: 45733.13, Avg. batch: 3.52e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.46it/s]\n",
      "Epoch 116/1000 (Loss - Running: 45652.11, Avg. batch: 3.51e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 230.52it/s]\n",
      "Epoch 117/1000 (Loss - Running: 46584.43, Avg. batch: 3.58e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 227.07it/s]\n",
      "Epoch 118/1000 (Loss - Running: 45503.04, Avg. batch: 3.50e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.94it/s]\n",
      "Epoch 119/1000 (Loss - Running: 45876.03, Avg. batch: 3.53e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 229.77it/s]\n",
      "Epoch 120/1000 (Loss - Running: 46345.40, Avg. batch: 3.57e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.20it/s]\n",
      "Epoch 121/1000 (Loss - Running: 45429.11, Avg. batch: 3.49e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.64it/s]\n",
      "Epoch 122/1000 (Loss - Running: 45785.65, Avg. batch: 3.52e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.28it/s]\n",
      "Epoch 123/1000 (Loss - Running: 45457.40, Avg. batch: 3.50e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.82it/s]\n",
      "Epoch 124/1000 (Loss - Running: 44639.65, Avg. batch: 3.43e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.02it/s]\n",
      "Epoch 125/1000 (Loss - Running: 44528.93, Avg. batch: 3.43e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 231.56it/s]\n",
      "Epoch 126/1000 (Loss - Running: 44928.07, Avg. batch: 3.46e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 230.39it/s]\n",
      "Epoch 127/1000 (Loss - Running: 44768.39, Avg. batch: 3.44e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.21it/s]\n",
      "Epoch 128/1000 (Loss - Running: 44290.30, Avg. batch: 3.41e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.45it/s]\n",
      "Epoch 129/1000 (Loss - Running: 44765.10, Avg. batch: 3.44e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 230.27it/s]\n",
      "Epoch 130/1000 (Loss - Running: 45232.83, Avg. batch: 3.48e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.82it/s]\n",
      "Epoch 131/1000 (Loss - Running: 43992.31, Avg. batch: 3.38e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.41it/s]\n",
      "Epoch 132/1000 (Loss - Running: 44639.37, Avg. batch: 3.43e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 228.09it/s]\n",
      "Epoch 133/1000 (Loss - Running: 44806.11, Avg. batch: 3.45e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.02it/s]\n",
      "Epoch 134/1000 (Loss - Running: 45119.74, Avg. batch: 3.47e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.16it/s]\n",
      "Epoch 135/1000 (Loss - Running: 44552.40, Avg. batch: 3.43e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.61it/s]\n",
      "Epoch 136/1000 (Loss - Running: 45971.31, Avg. batch: 3.54e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.49it/s]\n",
      "Epoch 137/1000 (Loss - Running: 44055.22, Avg. batch: 3.39e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.36it/s]\n",
      "Epoch 138/1000 (Loss - Running: 44922.41, Avg. batch: 3.46e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.05it/s]\n",
      "Epoch 139/1000 (Loss - Running: 44750.72, Avg. batch: 3.44e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 231.02it/s]\n",
      "Epoch 140/1000 (Loss - Running: 44894.74, Avg. batch: 3.45e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.14it/s]\n",
      "Epoch 141/1000 (Loss - Running: 44521.86, Avg. batch: 3.42e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.17it/s]\n",
      "Epoch 142/1000 (Loss - Running: 44470.83, Avg. batch: 3.42e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.78it/s]\n",
      "Epoch 143/1000 (Loss - Running: 43351.59, Avg. batch: 3.33e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 230.77it/s]\n",
      "Epoch 144/1000 (Loss - Running: 44054.26, Avg. batch: 3.39e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.59it/s]\n",
      "Epoch 145/1000 (Loss - Running: 44339.89, Avg. batch: 3.41e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.05it/s]\n",
      "Epoch 146/1000 (Loss - Running: 44540.54, Avg. batch: 3.43e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 230.15it/s]\n",
      "Epoch 147/1000 (Loss - Running: 44350.93, Avg. batch: 3.41e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 228.89it/s]\n",
      "Epoch 148/1000 (Loss - Running: 43823.81, Avg. batch: 3.37e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 227.15it/s]\n",
      "Epoch 149/1000 (Loss - Running: 43332.54, Avg. batch: 3.33e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 235.36it/s]\n",
      "Epoch 150/1000 (Loss - Running: 43932.30, Avg. batch: 3.38e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.67it/s]\n",
      "Epoch 151/1000 (Loss - Running: 43691.02, Avg. batch: 3.36e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.67it/s]\n",
      "Epoch 152/1000 (Loss - Running: 44293.20, Avg. batch: 3.41e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.01it/s]\n",
      "Epoch 153/1000 (Loss - Running: 44718.27, Avg. batch: 3.44e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.59it/s]\n",
      "Epoch 154/1000 (Loss - Running: 45581.46, Avg. batch: 3.51e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.69it/s]\n",
      "Epoch 155/1000 (Loss - Running: 45258.42, Avg. batch: 3.48e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.40it/s]\n",
      "Epoch 156/1000 (Loss - Running: 45056.97, Avg. batch: 3.47e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 230.49it/s]\n",
      "Epoch 157/1000 (Loss - Running: 44533.65, Avg. batch: 3.43e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 230.71it/s]\n",
      "Epoch 158/1000 (Loss - Running: 44473.29, Avg. batch: 3.42e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.60it/s]\n",
      "Epoch 159/1000 (Loss - Running: 44242.76, Avg. batch: 3.40e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.54it/s]\n",
      "Epoch 160/1000 (Loss - Running: 45431.70, Avg. batch: 3.49e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.10it/s]\n",
      "Epoch 161/1000 (Loss - Running: 43739.58, Avg. batch: 3.36e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 231.86it/s]\n",
      "Epoch 162/1000 (Loss - Running: 43072.12, Avg. batch: 3.31e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.30it/s]\n",
      "Epoch 163/1000 (Loss - Running: 43016.42, Avg. batch: 3.31e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.45it/s]\n",
      "Epoch 164/1000 (Loss - Running: 43119.90, Avg. batch: 3.32e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 234.28it/s]\n",
      "Epoch 165/1000 (Loss - Running: 41229.61, Avg. batch: 3.17e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 231.97it/s]\n",
      "Epoch 166/1000 (Loss - Running: 40984.73, Avg. batch: 3.15e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 230.83it/s]\n",
      "Epoch 167/1000 (Loss - Running: 43462.12, Avg. batch: 3.34e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 225.57it/s]\n",
      "Epoch 168/1000 (Loss - Running: 39486.05, Avg. batch: 3.04e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.27it/s]\n",
      "Epoch 169/1000 (Loss - Running: 37943.53, Avg. batch: 2.92e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 231.92it/s]\n",
      "Epoch 170/1000 (Loss - Running: 39365.07, Avg. batch: 3.03e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 231.12it/s]\n",
      "Epoch 171/1000 (Loss - Running: 37879.64, Avg. batch: 2.91e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 230.79it/s]\n",
      "Epoch 172/1000 (Loss - Running: 37472.20, Avg. batch: 2.88e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 231.47it/s]\n",
      "Epoch 173/1000 (Loss - Running: 45096.02, Avg. batch: 3.47e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.25it/s]\n",
      "Epoch 174/1000 (Loss - Running: 37588.90, Avg. batch: 2.89e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 233.41it/s]\n",
      "Epoch 175/1000 (Loss - Running: 36391.55, Avg. batch: 2.80e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 231.56it/s]\n",
      "Epoch 176/1000 (Loss - Running: 39705.01, Avg. batch: 3.05e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 229.47it/s]\n",
      "Epoch 177/1000 (Loss - Running: 36662.50, Avg. batch: 2.82e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 228.34it/s]\n",
      "Epoch 178/1000 (Loss - Running: 37906.30, Avg. batch: 2.92e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 229.65it/s]\n",
      "Epoch 179/1000 (Loss - Running: 38010.47, Avg. batch: 2.92e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 230.61it/s]\n",
      "Epoch 180/1000 (Loss - Running: 39579.81, Avg. batch: 3.04e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 230.97it/s]\n",
      "Epoch 181/1000 (Loss - Running: 37993.74, Avg. batch: 2.92e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 235.06it/s]\n",
      "Epoch 182/1000 (Loss - Running: 34575.94, Avg. batch: 2.66e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.83it/s]\n",
      "Epoch 183/1000 (Loss - Running: 35472.82, Avg. batch: 2.73e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.44it/s]\n",
      "Epoch 184/1000 (Loss - Running: 33945.45, Avg. batch: 2.61e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.94it/s]\n",
      "Epoch 185/1000 (Loss - Running: 37806.07, Avg. batch: 2.91e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 232.16it/s]\n",
      "Epoch 186/1000 (Loss - Running: 36954.75, Avg. batch: 2.84e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 227.81it/s]\n",
      "Epoch 187/1000 (Loss - Running: 35907.68, Avg. batch: 2.76e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 226.47it/s]\n",
      "Epoch 188/1000 (Loss - Running: 36955.46, Avg. batch: 2.84e+01, Best avg.: 25.38): 100%|██████████| 1304/1304 [00:05<00:00, 228.03it/s]\n",
      "Epoch 189/1000 (Loss - Running: 36496.31, Avg. batch: 2.81e+01, Best avg.: 25.37): 100%|██████████| 1304/1304 [00:05<00:00, 232.48it/s]\n",
      "Epoch 190/1000 (Loss - Running: 35789.49, Avg. batch: 2.75e+01, Best avg.: 25.37): 100%|██████████| 1304/1304 [00:05<00:00, 231.63it/s]\n",
      "Epoch 191/1000 (Loss - Running: 38874.68, Avg. batch: 2.99e+01, Best avg.: 25.37): 100%|██████████| 1304/1304 [00:05<00:00, 232.83it/s]\n",
      "Epoch 192/1000 (Loss - Running: 37595.16, Avg. batch: 2.89e+01, Best avg.: 25.37): 100%|██████████| 1304/1304 [00:05<00:00, 230.48it/s]\n",
      "Epoch 193/1000 (Loss - Running: 36189.73, Avg. batch: 2.78e+01, Best avg.: 25.37): 100%|██████████| 1304/1304 [00:05<00:00, 232.67it/s]\n",
      "Epoch 194/1000 (Loss - Running: 34363.45, Avg. batch: 2.64e+01, Best avg.: 25.37): 100%|██████████| 1304/1304 [00:05<00:00, 232.16it/s]\n",
      "Epoch 195/1000 (Loss - Running: 38105.93, Avg. batch: 2.93e+01, Best avg.: 25.37): 100%|██████████| 1304/1304 [00:05<00:00, 233.14it/s]\n",
      "Epoch 196/1000 (Loss - Running: 36741.12, Avg. batch: 2.83e+01, Best avg.: 25.37): 100%|██████████| 1304/1304 [00:05<00:00, 230.23it/s]\n",
      "Epoch 197/1000 (Loss - Running: 35407.41, Avg. batch: 2.72e+01, Best avg.: 25.37): 100%|██████████| 1304/1304 [00:05<00:00, 218.35it/s]\n",
      "Epoch 198/1000 (Loss - Running: 36020.57, Avg. batch: 2.77e+01, Best avg.: 25.37): 100%|██████████| 1304/1304 [00:05<00:00, 228.30it/s]\n",
      "Epoch 199/1000 (Loss - Running: 37900.91, Avg. batch: 2.92e+01, Best avg.: 25.37): 100%|██████████| 1304/1304 [00:05<00:00, 229.45it/s]\n",
      "Epoch 200/1000 (Loss - Running: 35667.18, Avg. batch: 2.74e+01, Best avg.: 25.37): 100%|██████████| 1304/1304 [00:05<00:00, 234.16it/s]\n",
      "Epoch 201/1000 (Loss - Running: 36978.74, Avg. batch: 2.84e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 231.12it/s]\n",
      "Epoch 202/1000 (Loss - Running: 35994.67, Avg. batch: 2.77e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 231.35it/s]\n",
      "Epoch 203/1000 (Loss - Running: 35423.13, Avg. batch: 2.72e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 231.78it/s]\n",
      "Epoch 204/1000 (Loss - Running: 36805.25, Avg. batch: 2.83e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 231.97it/s]\n",
      "Epoch 205/1000 (Loss - Running: 39373.70, Avg. batch: 3.03e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 229.93it/s]\n",
      "Epoch 206/1000 (Loss - Running: 37344.93, Avg. batch: 2.87e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 230.14it/s]\n",
      "Epoch 207/1000 (Loss - Running: 34555.22, Avg. batch: 2.66e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 231.29it/s]\n",
      "Epoch 208/1000 (Loss - Running: 36028.55, Avg. batch: 2.77e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 231.12it/s]\n",
      "Epoch 209/1000 (Loss - Running: 35590.48, Avg. batch: 2.74e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 233.30it/s]\n",
      "Epoch 210/1000 (Loss - Running: 38542.30, Avg. batch: 2.96e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 233.53it/s]\n",
      "Epoch 211/1000 (Loss - Running: 36230.85, Avg. batch: 2.79e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 230.93it/s]\n",
      "Epoch 212/1000 (Loss - Running: 34125.11, Avg. batch: 2.63e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 232.15it/s]\n",
      "Epoch 213/1000 (Loss - Running: 35284.29, Avg. batch: 2.71e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 231.98it/s]\n",
      "Epoch 214/1000 (Loss - Running: 35974.55, Avg. batch: 2.77e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 229.94it/s]\n",
      "Epoch 215/1000 (Loss - Running: 36817.15, Avg. batch: 2.83e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 230.17it/s]\n",
      "Epoch 216/1000 (Loss - Running: 35325.70, Avg. batch: 2.72e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 227.73it/s]\n",
      "Epoch 217/1000 (Loss - Running: 39449.32, Avg. batch: 3.03e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 228.40it/s]\n",
      "Epoch 218/1000 (Loss - Running: 34517.99, Avg. batch: 2.66e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 233.07it/s]\n",
      "Epoch 219/1000 (Loss - Running: 39109.59, Avg. batch: 3.01e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 231.49it/s]\n",
      "Epoch 220/1000 (Loss - Running: 34904.89, Avg. batch: 2.68e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 232.51it/s]\n",
      "Epoch 221/1000 (Loss - Running: 36784.21, Avg. batch: 2.83e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 230.98it/s]\n",
      "Epoch 222/1000 (Loss - Running: 34036.46, Avg. batch: 2.62e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 231.48it/s]\n",
      "Epoch 223/1000 (Loss - Running: 35356.62, Avg. batch: 2.72e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 230.87it/s]\n",
      "Epoch 224/1000 (Loss - Running: 38081.24, Avg. batch: 2.93e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 230.88it/s]\n",
      "Epoch 225/1000 (Loss - Running: 35390.77, Avg. batch: 2.72e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 227.95it/s]\n",
      "Epoch 226/1000 (Loss - Running: 34651.98, Avg. batch: 2.67e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 222.57it/s]\n",
      "Epoch 227/1000 (Loss - Running: 34902.10, Avg. batch: 2.68e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 229.32it/s]\n",
      "Epoch 228/1000 (Loss - Running: 35058.18, Avg. batch: 2.70e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 230.52it/s]\n",
      "Epoch 229/1000 (Loss - Running: 36896.79, Avg. batch: 2.84e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 230.83it/s]\n",
      "Epoch 230/1000 (Loss - Running: 35241.27, Avg. batch: 2.71e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 232.66it/s]\n",
      "Epoch 231/1000 (Loss - Running: 36508.31, Avg. batch: 2.81e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 232.63it/s]\n",
      "Epoch 232/1000 (Loss - Running: 35960.70, Avg. batch: 2.77e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 229.81it/s]\n",
      "Epoch 233/1000 (Loss - Running: 33933.94, Avg. batch: 2.61e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 230.68it/s]\n",
      "Epoch 234/1000 (Loss - Running: 33053.60, Avg. batch: 2.54e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 228.86it/s]\n",
      "Epoch 235/1000 (Loss - Running: 43275.30, Avg. batch: 3.33e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 228.19it/s]\n",
      "Epoch 236/1000 (Loss - Running: 39026.74, Avg. batch: 3.00e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 227.55it/s]\n",
      "Epoch 237/1000 (Loss - Running: 34596.15, Avg. batch: 2.66e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 228.91it/s]\n",
      "Epoch 238/1000 (Loss - Running: 34577.68, Avg. batch: 2.66e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 232.67it/s]\n",
      "Epoch 239/1000 (Loss - Running: 37257.24, Avg. batch: 2.87e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 232.06it/s]\n",
      "Epoch 240/1000 (Loss - Running: 36633.15, Avg. batch: 2.82e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 231.65it/s]\n",
      "Epoch 241/1000 (Loss - Running: 34451.31, Avg. batch: 2.65e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 228.30it/s]\n",
      "Epoch 242/1000 (Loss - Running: 41709.47, Avg. batch: 3.21e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 232.25it/s]\n",
      "Epoch 243/1000 (Loss - Running: 36707.92, Avg. batch: 2.82e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 230.78it/s]\n",
      "Epoch 244/1000 (Loss - Running: 35848.69, Avg. batch: 2.76e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 232.42it/s]\n",
      "Epoch 245/1000 (Loss - Running: 33834.01, Avg. batch: 2.60e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 226.32it/s]\n",
      "Epoch 246/1000 (Loss - Running: 33928.54, Avg. batch: 2.61e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 227.45it/s]\n",
      "Epoch 247/1000 (Loss - Running: 33706.24, Avg. batch: 2.59e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 231.42it/s]\n",
      "Epoch 248/1000 (Loss - Running: 33152.57, Avg. batch: 2.55e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 230.74it/s]\n",
      "Epoch 249/1000 (Loss - Running: 38027.23, Avg. batch: 2.93e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 232.12it/s]\n",
      "Epoch 250/1000 (Loss - Running: 36333.37, Avg. batch: 2.79e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 231.00it/s]\n",
      "Epoch 251/1000 (Loss - Running: 36297.59, Avg. batch: 2.79e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 230.76it/s]\n",
      "Epoch 252/1000 (Loss - Running: 35343.07, Avg. batch: 2.72e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 232.67it/s]\n",
      "Epoch 253/1000 (Loss - Running: 34559.93, Avg. batch: 2.66e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 232.81it/s]\n",
      "Epoch 254/1000 (Loss - Running: 34395.89, Avg. batch: 2.65e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 230.60it/s]\n",
      "Epoch 255/1000 (Loss - Running: 41387.40, Avg. batch: 3.18e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 224.88it/s]\n",
      "Epoch 256/1000 (Loss - Running: 38836.27, Avg. batch: 2.99e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 228.09it/s]\n",
      "Epoch 257/1000 (Loss - Running: 34166.53, Avg. batch: 2.63e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 230.08it/s]\n",
      "Epoch 258/1000 (Loss - Running: 33591.71, Avg. batch: 2.58e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 230.12it/s]\n",
      "Epoch 259/1000 (Loss - Running: 33412.78, Avg. batch: 2.57e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 230.12it/s]\n",
      "Epoch 260/1000 (Loss - Running: 35323.87, Avg. batch: 2.72e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 227.02it/s]\n",
      "Epoch 261/1000 (Loss - Running: 34592.21, Avg. batch: 2.66e+01, Best avg.: 24.97): 100%|██████████| 1304/1304 [00:05<00:00, 229.89it/s]\n",
      "Epoch 262/1000 (Loss - Running: 23173.16, Avg. batch: 2.90e+01, Best avg.: 24.97):  64%|██████▍   | 837/1304 [00:03<00:01, 235.52it/s]"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "BBM_creator.train(save_to=\"models\", epochs=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T02:28:29.447012800Z",
     "start_time": "2024-02-01T00:50:01.947639800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "BBM_creator._summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
